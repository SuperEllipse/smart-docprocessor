{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/cdsw/utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 38 key-value pairs and 339 tensors from /home/cdsw/.cache/huggingface/hub/models--bartowski--Qwen2.5-7B-Instruct-GGUF/snapshots/8911e8a47f92bac19d6f5c64a2e2095bd2f7d031/./Qwen2.5-7B-Instruct-Q6_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...\n",
      "llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B\n",
      "llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B\n",
      "llama_model_loader: - kv  12:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
      "llama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                      quantize.imatrix.file str              = /models_out/Qwen2.5-7B-Instruct-GGUF/...\n",
      "llama_model_loader: - kv  35:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  36:             quantize.imatrix.entries_count i32              = 196\n",
      "llama_model_loader: - kv  37:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q8_0:    2 tensors\n",
      "llama_model_loader: - type q6_K:  196 tensors\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.9310 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 152064\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 3584\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 28\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 18944\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.62 B\n",
      "llm_load_print_meta: model size       = 6.06 GiB (6.84 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen2.5 7B Instruct\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  6210.54 MiB\n",
      ".....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 16384\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   896.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   956.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 986\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.entries_count': '196', 'quantize.imatrix.file': '/models_out/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct.imatrix', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-7B', 'general.license': 'apache-2.0', 'qwen2.attention.head_count_kv': '4', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'general.type': 'model', 'qwen2.block_count': '28', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.base_model.0.name': 'Qwen2.5 7B', 'general.license.link': 'https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Qwen', 'general.size_label': '7B', 'tokenizer.ggml.add_bos_token': 'false', 'general.basename': 'Qwen2.5', 'qwen2.embedding_length': '3584', 'tokenizer.ggml.padding_token_id': '151643', 'general.architecture': 'qwen2', 'qwen2.context_length': '32768', 'qwen2.feed_forward_length': '18944', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'qwen2.attention.head_count': '28', 'qwen2.rope.freq_base': '1000000.000000', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.finetune': 'Instruct', 'general.file_type': '18', 'general.name': 'Qwen2.5 7B Instruct', 'tokenizer.ggml.bos_token_id': '151643'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from copy import deepcopy\n",
    "from smartdoc_utils import process_llm_output\n",
    "from deterministic_preprocessor import DeterministicPreprocessor\n",
    "from prompts import (\n",
    "    karen_prompt,\n",
    "    karen_system_prompt,\n",
    "    llama_prompt,\n",
    "    llama_system_prompt,\n",
    "    gemma_prompt,\n",
    ")\n",
    "from llm_configs import karen_config, llama_config, gemma_config\n",
    "\n",
    "# Initialize the model with GPU support\n",
    "# llm = Llama(\n",
    "#     model_path=settings.llm_model.model_name,\n",
    "#     n_gpu_layers=-1,  # -1 means use all available GPU layers\n",
    "#     n_ctx=16384,  # adjust based on your GPU memory\n",
    "# )\n",
    "\n",
    "# karen 7b q6_k creative\n",
    "# llm = Llama.from_pretrained(\n",
    "#     repo_id=\"FPHam/Karen_TheEditor_V2_CREATIVE_Mistral_7B-Q6_K-GGUF\",\n",
    "#     filename=\"karen_theeditor_v2_creative_mistral_7b.Q6_K.gguf\",\n",
    "#     n_gpu_layers=-1,\n",
    "#     n_ctx=16384,\n",
    "# )\n",
    "\n",
    "# karen 7b q6_k strict\n",
    "# llm = Llama.from_pretrained(\n",
    "#     repo_id=\"TheBloke/Karen_TheEditor_V2_STRICT_Mistral_7B-GGUF\",\n",
    "#     filename=\"karen_theeditor_v2_strict_mistral_7b.Q6_K.gguf\",\n",
    "#     n_gpu_layers=-1,\n",
    "#     n_ctx=16384,\n",
    "# )\n",
    "\n",
    "# llama 3.1 8b q6_k_l\n",
    "# llm = Llama.from_pretrained(\n",
    "#     repo_id=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "#     filename=\"Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf\",\n",
    "#     n_gpu_layers=-1,\n",
    "#     n_ctx=16384,\n",
    "# )\n",
    "\n",
    "# llama 3.2 3b fp16\n",
    "# llm = Llama.from_pretrained(\n",
    "#     repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n",
    "#     filename=\"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "#     n_gpu_layers=-1,\n",
    "#     n_ctx=16384,\n",
    "# )\n",
    "\n",
    "# gemma 2 9 b q6_k_l\n",
    "# llm = Llama.from_pretrained(\n",
    "#     repo_id=\"bartowski/gemma-2-9b-it-GGUF\",\n",
    "#     filename=\"gemma-2-9b-it-Q6_K_L.gguf\",\n",
    "#     n_gpu_layers=-1,\n",
    "#     n_ctx=8192,\n",
    "# )\n",
    "\n",
    "# qwen2.5 7b q6_k_l\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"bartowski/Qwen2.5-7B-Instruct-GGUF\",\n",
    "    filename=\"Qwen2.5-7B-Instruct-Q6_K_L.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=16384,\n",
    ")\n",
    "\n",
    "if \"karen\" in llm.model_path:\n",
    "    generation_params = karen_config\n",
    "    PROMPT_TEMPLATE = karen_prompt\n",
    "    SYS_PROMPT = karen_system_prompt\n",
    "elif \"gemma\" in llm.model_path:\n",
    "    generation_params = gemma_config\n",
    "    PROMPT_TEMPLATE = gemma_prompt\n",
    "    SYS_PROMPT = None\n",
    "else:\n",
    "    generation_params = llama_config\n",
    "    PROMPT_TEMPLATE = llama_prompt\n",
    "    SYS_PROMPT = llama_system_prompt\n",
    "\n",
    "\n",
    "def fetch_llm_response(\n",
    "    text,\n",
    "    prompt_template=PROMPT_TEMPLATE,\n",
    "    sys_prompt=SYS_PROMPT,\n",
    "):\n",
    "    if sys_prompt is None:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_template.format(text=text),\n",
    "            },\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_template.format(text=text),\n",
    "            },\n",
    "        ]\n",
    "    edited_text = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        **generation_params,\n",
    "    )\n",
    "    response_text = edited_text[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    print(f\"PROMPT: \\n {prompt_template.format(text=text)} \\n\\n\")\n",
    "    print(f\"LLM RESPONSE: \\n{response_text}\\n\\n\")\n",
    "\n",
    "    return response_text\n",
    "\n",
    "\n",
    "# language checks, etc. before further processing\n",
    "#\n",
    "# Args:\n",
    "#   input_doc: The original input document\n",
    "#   corrections_doc: An empty document to populate with corrections\n",
    "#\n",
    "# Returns:\n",
    "#   modified_doc: A modified version of the input document\n",
    "#   corrections_doc: Populated with any corrections made during p\n",
    "def pre_process_document(input_doc, corrections_doc):\n",
    "\n",
    "    # logic for pre-processing Fonts, Australian Language Checks, etc.\n",
    "\n",
    "    # -- Need to uncomment following 3 lines - the code has not been tested here yet!\n",
    "\n",
    "    preprocessor = DeterministicPreprocessor()\n",
    "    modified_doc, corrections_doc = preprocessor.pre_process_document(input_doc)\n",
    "\n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "\n",
    "def process_document_paragraphs(modified_doc, corrections_doc):\n",
    "\n",
    "    # modified_doc = deepcopy(input_doc)\n",
    "    # Process each paragraph\n",
    "    for para in modified_doc.paragraphs:\n",
    "        # Store the original formatting\n",
    "        original_runs = para.runs.copy()\n",
    "\n",
    "        if not para.text.strip():\n",
    "            print(\"Skipping LLM CALL:\")\n",
    "            continue\n",
    "        else:\n",
    "            # Get the text content\n",
    "            text = para.text\n",
    "\n",
    "        llm_output_text = fetch_llm_response(text)\n",
    "\n",
    "        # edits, corrections = document_postprocessing(llm_output_text)\n",
    "        edits, corrections = process_llm_output(llm_output_text)\n",
    "        print(\"EDITS : \\n\", edits)\n",
    "        print(\"CORRECTIONS: \\n\", corrections)\n",
    "        # Clear the paragraph and add the edited text\n",
    "        para.clear()\n",
    "        # para.add_run(edited_text['choices'][0]['text'].strip())\n",
    "        para.add_run(edits)\n",
    "        # Attempt to reapply formatting\n",
    "        new_runs = para.runs\n",
    "        for i, run in enumerate(new_runs):\n",
    "            if i < len(original_runs):\n",
    "                run.font.name = original_runs[i].font.name\n",
    "                run.font.size = original_runs[i].font.size\n",
    "                run.font.bold = original_runs[i].font.bold\n",
    "                run.font.italic = original_runs[i].font.italic\n",
    "                run.font.color.rgb = original_runs[i].font.color.rgb\n",
    "                run.font.underline = original_runs[i].font.underline\n",
    "                # Add more formatting attributes as needed\n",
    "\n",
    "        # Let us log the corrections made\n",
    "        corrections_doc.add_paragraph()\n",
    "        corrections_doc.add_paragraph(f\"Original Text : \\n {text}\")\n",
    "        corrections_doc.add_paragraph(f\"Edits : \\n {edits}\")\n",
    "        corrections_doc.add_paragraph(f\"Corrections: \\n{llm_output_text}\")\n",
    "\n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "\n",
    "def process_document_tables(modified_doc, corrections_doc):\n",
    "\n",
    "    ## REPLACE this with logic of the modified doc\n",
    "    # modified_doc = deepcopy(input_doc)\n",
    "    # Iterate through all tables in the document\n",
    "    for table in modified_doc.tables:\n",
    "        print(\"IN Table\")\n",
    "        printed_cells = set()  # To keep track of cells that have been processed\n",
    "        for r_index, row in enumerate(table.rows):\n",
    "            for c_index, cell in enumerate(row.cells):\n",
    "                cell_id = (r_index, c_index)  # Unique identifier for the cell\n",
    "\n",
    "                # Skip this cell if it is already processed as part of a merged cell\n",
    "                if cell_id in printed_cells:\n",
    "                    continue\n",
    "\n",
    "                # Detect merged cells\n",
    "                is_merged = False\n",
    "                for other_cell in row.cells:\n",
    "                    if other_cell is not cell and other_cell._element is cell._element:\n",
    "                        is_merged = True\n",
    "                        break\n",
    "\n",
    "                # If it's a merged cell, avoid processing duplicates\n",
    "                if is_merged:\n",
    "                    # Register this cell's element to skip duplicates\n",
    "                    for merged_row_index, merged_row in enumerate(table.rows):\n",
    "                        for merged_cell_index, merged_cell in enumerate(\n",
    "                            merged_row.cells\n",
    "                        ):\n",
    "                            if merged_cell._element is cell._element:\n",
    "                                printed_cells.add((merged_row_index, merged_cell_index))\n",
    "\n",
    "                # Append '**' to the text of the cell if not already processed\n",
    "                if cell.text.strip():  # Check if the cell is not empty\n",
    "                    #                    cell.text += '*T*B*L'\n",
    "                    for para in cell.paragraphs:\n",
    "                        # Add an asterisk (*) to the end of each cell paragraph\n",
    "                        print(para.text)\n",
    "                        # Just a small check to see that we processed this\n",
    "                        # para.add_run('*T')\n",
    "\n",
    "                        # Store the original formatting\n",
    "                        original_runs = para.runs.copy()\n",
    "                        # let us call the llm\n",
    "                        llm_output_text = fetch_llm_response(para.text)\n",
    "\n",
    "                        # edits, corrections = document_postprocessing(llm_output_text)\n",
    "                        edits, corrections = process_llm_output(llm_output_text)\n",
    "\n",
    "                        # let us reapply formatting\n",
    "\n",
    "                        # Clear the paragraph and add the edited text\n",
    "                        para.clear()\n",
    "                        # para.add_run(edited_text['choices'][0]['text'].strip())\n",
    "                        para.add_run(edits)\n",
    "                        # Attempt to reapply formatting\n",
    "                        new_runs = para.runs\n",
    "                        for i, run in enumerate(new_runs):\n",
    "                            if i < len(original_runs):\n",
    "                                run.font.name = original_runs[i].font.name\n",
    "                                run.font.size = original_runs[i].font.size\n",
    "                                run.font.bold = original_runs[i].font.bold\n",
    "                                run.font.italic = original_runs[i].font.italic\n",
    "                                run.font.color.rgb = original_runs[i].font.color.rgb\n",
    "                                run.font.underline = original_runs[i].font.underline\n",
    "                                # Add more formatting attributes as needed\n",
    "\n",
    "                        # Let us log the corrections made\n",
    "                        corrections_doc.add_paragraph()\n",
    "                        corrections_doc.add_paragraph(f\"Original Text : \\n {para.text}\")\n",
    "                        corrections_doc.add_paragraph(\n",
    "                            f\"Corrections: \\n{llm_output_text}\"\n",
    "                        )\n",
    "\n",
    "            print()  # Newline after each row\n",
    "\n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "\n",
    "def proofread_and_correct_document(modified_doc, corrections_doc):\n",
    "    corrections_doc.add_heading(\"Corrections Made\", 0)\n",
    "\n",
    "    # Correct using deterministic grammar checking\n",
    "    modified_doc, corrections_doc = pre_process_document(modified_doc, corrections_doc)\n",
    "    modified_doc, corrections_doc = process_document_paragraphs(\n",
    "        modified_doc, corrections_doc\n",
    "    )\n",
    "    modified_doc, corrections_doc = process_document_tables(\n",
    "        modified_doc, corrections_doc\n",
    "    )\n",
    "\n",
    "    return modified_doc, corrections_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new document located in: /home/cdsw/data/Training MC for QA clean_output_trackchanges.docx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def build_file_name_extensions(input_file_path, extension=\"temp\"):\n",
    "    \"\"\"\n",
    "    Extracts the file name from a given path and creates new paths with specified suffixes.\n",
    "\n",
    "    Args:\n",
    "        input_file_path (str): The path to the file.\n",
    "        extension(str): name of the extension required\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the extracted file name, edited path, corrected path, and track changes path.\n",
    "\n",
    "    ## USAGE\n",
    "    orig_filename, new_filename, new_filename_with_path  = build_file_name_extensions(\"/home/cdsw/test.doc\", \"corrections\") \n",
    "    print(orig_filename, new_filename, new_filename_with_path ) # returns test.doc, test_corrections.doc, /home/cdsw/test_corrections.doc\n",
    "        \n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(input_file_path)\n",
    "    file_name_wo_ext = os.path.splitext(file_name)[0]\n",
    "    file_ext = os.path.splitext(file_name)[1]  # Get the file extension\n",
    "    new_file_name = f\"{file_name_wo_ext}_{extension}{file_ext}\"\n",
    "    new_file_path = os.path.join(os.path.dirname(input_file_path),new_file_name )\n",
    "    return file_name, new_file_name, new_file_path\n",
    "\n",
    "def create_track_changes_document(input_doc, edited_doc):\n",
    "    doc = Document(input_doc)\n",
    "    \n",
    "    # need a temp compy because input gets deleted\n",
    "    temp_doc = deepcopy(doc)\n",
    "    _,_, temp_input_doc = build_file_name_extensions(input_doc, \"temp\")\n",
    "    temp_doc.save(temp_input_doc)\n",
    "    \n",
    "    # need a temp copy of edit\n",
    "    doc = Document(edited_doc)\n",
    "    # need a temp compy because input gets deleted\n",
    "    temp_doc = deepcopy(doc)\n",
    "    _,_, temp_edited_doc = build_file_name_extensions(edited_doc, \"temp\")\n",
    "    temp_doc.save(temp_edited_doc)\n",
    "    \n",
    "    # apply track changes\n",
    "    \n",
    "    from python_redlines.engines import XmlPowerToolsEngine\n",
    "    wrapper = XmlPowerToolsEngine()\n",
    "\n",
    "    output_trackchanges = wrapper.run_redline('Smartdoc Processor', temp_input_doc, temp_edited_doc)\n",
    "    _,_, trackchanges_doc = build_file_name_extensions(edited_doc, \"trackchanges\")\n",
    "    with open(trackchanges_doc, 'wb') as f:\n",
    "        f.write(output_trackchanges[0])   \n",
    "    \n",
    "    return trackchanges_doc\n",
    "\n",
    "input_path = \"/home/cdsw/data/Training MC for QA clean.docx\"\n",
    "\n",
    "_,_, edit_path = build_file_name_extensions(input_path, extension=\"output\")\n",
    "_,_, correction_path = build_file_name_extensions(input_path, extension=\"corrections\")\n",
    "input_doc = Document(input_path)\n",
    "modified_doc = deepcopy(input_doc)\n",
    "# Create a document object for corrections\n",
    "corrections_doc = Document()\n",
    "modified_doc, corrections_doc = proofread_and_correct_document(\n",
    "    modified_doc, corrections_doc\n",
    ")\n",
    "modified_doc.save(edit_path)\n",
    "corrections_doc.save(correction_path)\n",
    "\n",
    "\n",
    "\n",
    "new_doc = create_track_changes_document(input_path, edit_path)\n",
    "print('new document located in:', new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
