{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 38 key-value pairs and 339 tensors from /home/cdsw/.cache/huggingface/hub/models--bartowski--Qwen2.5-7B-Instruct-GGUF/snapshots/8911e8a47f92bac19d6f5c64a2e2095bd2f7d031/./Qwen2.5-7B-Instruct-Q6_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...\n",
      "llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B\n",
      "llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B\n",
      "llama_model_loader: - kv  12:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
      "llama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                      quantize.imatrix.file str              = /models_out/Qwen2.5-7B-Instruct-GGUF/...\n",
      "llama_model_loader: - kv  35:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  36:             quantize.imatrix.entries_count i32              = 196\n",
      "llama_model_loader: - kv  37:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q8_0:    2 tensors\n",
      "llama_model_loader: - type q6_K:  196 tensors\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.9310 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 152064\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 3584\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 28\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 18944\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.62 B\n",
      "llm_load_print_meta: model size       = 6.06 GiB (6.84 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen2.5 7B Instruct\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  6210.54 MiB\n",
      ".....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 16384\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   896.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   956.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 986\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.entries_count': '196', 'quantize.imatrix.file': '/models_out/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct.imatrix', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-7B', 'general.license': 'apache-2.0', 'qwen2.attention.head_count_kv': '4', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'general.type': 'model', 'qwen2.block_count': '28', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.base_model.0.name': 'Qwen2.5 7B', 'general.license.link': 'https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Qwen', 'general.size_label': '7B', 'tokenizer.ggml.add_bos_token': 'false', 'general.basename': 'Qwen2.5', 'qwen2.embedding_length': '3584', 'tokenizer.ggml.padding_token_id': '151643', 'general.architecture': 'qwen2', 'qwen2.context_length': '32768', 'qwen2.feed_forward_length': '18944', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'qwen2.attention.head_count': '28', 'qwen2.rope.freq_base': '1000000.000000', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.finetune': 'Instruct', 'general.file_type': '18', 'general.name': 'Qwen2.5 7B Instruct', 'tokenizer.ggml.bos_token_id': '151643'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from copy import deepcopy\n",
    "from smartdoc_utils import process_llm_output\n",
    "from deterministic_preprocessor import DeterministicPreprocessor\n",
    "from prompts import (\n",
    "    karen_prompt,\n",
    "    karen_system_prompt,\n",
    "    llama_prompt,\n",
    "    llama_system_prompt,\n",
    "    gemma_prompt,\n",
    ")\n",
    "from llm_configs import karen_config, llama_config, gemma_config\n",
    "\n",
    "# Initialize the model with GPU support\n",
    "# llm = Llama(\n",
    "#     model_path=settings.llm_model.model_name,\n",
    "#     n_gpu_layers=-1,  # -1 means use all available GPU layers\n",
    "#     n_ctx=16384,  # adjust based on your GPU memory\n",
    "# )\n",
    "\n",
    "# karen 7b q6_k creative\n",
    "# llm = Llama.from_pretrained(\n",
    "#     repo_id=\"FPHam/Karen_TheEditor_V2_CREATIVE_Mistral_7B-Q6_K-GGUF\",\n",
    "#     filename=\"karen_theeditor_v2_creative_mistral_7b.Q6_K.gguf\",\n",
    "#     n_gpu_layers=-1,\n",
    "#     n_ctx=16384,\n",
    "# )\n",
    "\n",
    "# karen 7b q6_k strict\n",
    "# llm = Llama.from_pretrained(\n",
    "#     repo_id=\"TheBloke/Karen_TheEditor_V2_STRICT_Mistral_7B-GGUF\",\n",
    "#     filename=\"karen_theeditor_v2_strict_mistral_7b.Q6_K.gguf\",\n",
    "#     n_gpu_layers=-1,\n",
    "#     n_ctx=16384,\n",
    "# )\n",
    "\n",
    "# llama 3.1 8b q6_k_l\n",
    "# llm = Llama.from_pretrained(\n",
    "#     repo_id=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "#     filename=\"Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf\",\n",
    "#     n_gpu_layers=-1,\n",
    "#     n_ctx=16384,\n",
    "# )\n",
    "\n",
    "# llama 3.2 3b fp16\n",
    "# llm = Llama.from_pretrained(\n",
    "#     repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n",
    "#     filename=\"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "#     n_gpu_layers=-1,\n",
    "#     n_ctx=16384,\n",
    "# )\n",
    "\n",
    "# gemma 2 9 b q6_k_l\n",
    "# llm = Llama.from_pretrained(\n",
    "#     repo_id=\"bartowski/gemma-2-9b-it-GGUF\",\n",
    "#     filename=\"gemma-2-9b-it-Q6_K_L.gguf\",\n",
    "#     n_gpu_layers=-1,\n",
    "#     n_ctx=8192,\n",
    "# )\n",
    "\n",
    "# qwen2.5 7b q6_k_l\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"bartowski/Qwen2.5-7B-Instruct-GGUF\",\n",
    "    filename=\"Qwen2.5-7B-Instruct-Q6_K_L.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=16384,\n",
    ")\n",
    "\n",
    "if \"karen\" in llm.model_path:\n",
    "    generation_params = karen_config\n",
    "    PROMPT_TEMPLATE = karen_prompt\n",
    "    SYS_PROMPT = karen_system_prompt\n",
    "elif \"gemma\" in llm.model_path:\n",
    "    generation_params = gemma_config\n",
    "    PROMPT_TEMPLATE = gemma_prompt\n",
    "    SYS_PROMPT = None\n",
    "else:\n",
    "    generation_params = llama_config\n",
    "    PROMPT_TEMPLATE = llama_prompt\n",
    "    SYS_PROMPT = llama_system_prompt\n",
    "\n",
    "\n",
    "def fetch_llm_response(\n",
    "    text,\n",
    "    prompt_template=PROMPT_TEMPLATE,\n",
    "    sys_prompt=SYS_PROMPT,\n",
    "):\n",
    "    if sys_prompt is None:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_template.format(text=text),\n",
    "            },\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_template.format(text=text),\n",
    "            },\n",
    "        ]\n",
    "    edited_text = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        **generation_params,\n",
    "    )\n",
    "    response_text = edited_text[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    print(f\"PROMPT: \\n {prompt_template.format(text=text)} \\n\\n\")\n",
    "    print(f\"LLM RESPONSE: \\n{response_text}\\n\\n\")\n",
    "\n",
    "    return response_text\n",
    "\n",
    "\n",
    "# language checks, etc. before further processing\n",
    "#\n",
    "# Args:\n",
    "#   input_doc: The original input document\n",
    "#   corrections_doc: An empty document to populate with corrections\n",
    "#\n",
    "# Returns:\n",
    "#   modified_doc: A modified version of the input document\n",
    "#   corrections_doc: Populated with any corrections made during p\n",
    "def pre_process_document(input_doc, corrections_doc):\n",
    "\n",
    "    # logic for pre-processing Fonts, Australian Language Checks, etc.\n",
    "\n",
    "    # -- Need to uncomment following 3 lines - the code has not been tested here yet!\n",
    "\n",
    "    preprocessor = DeterministicPreprocessor()\n",
    "    modified_doc, corrections_doc = preprocessor.pre_process_document(input_doc)\n",
    "\n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "\n",
    "def process_document_paragraphs(modified_doc, corrections_doc):\n",
    "\n",
    "    # modified_doc = deepcopy(input_doc)\n",
    "    # Process each paragraph\n",
    "    for para in modified_doc.paragraphs:\n",
    "        # Store the original formatting\n",
    "        original_runs = para.runs.copy()\n",
    "\n",
    "        if not para.text.strip():\n",
    "            print(\"Skipping LLM CALL:\")\n",
    "            continue\n",
    "        else:\n",
    "            # Get the text content\n",
    "            text = para.text\n",
    "\n",
    "        llm_output_text = fetch_llm_response(text)\n",
    "\n",
    "        # edits, corrections = document_postprocessing(llm_output_text)\n",
    "        edits, corrections = process_llm_output(llm_output_text)\n",
    "        print(\"EDITS : \\n\", edits)\n",
    "        print(\"CORRECTIONS: \\n\", corrections)\n",
    "        # Clear the paragraph and add the edited text\n",
    "        para.clear()\n",
    "        # para.add_run(edited_text['choices'][0]['text'].strip())\n",
    "        para.add_run(edits)\n",
    "        # Attempt to reapply formatting\n",
    "        new_runs = para.runs\n",
    "        for i, run in enumerate(new_runs):\n",
    "            if i < len(original_runs):\n",
    "                run.font.name = original_runs[i].font.name\n",
    "                run.font.size = original_runs[i].font.size\n",
    "                run.font.bold = original_runs[i].font.bold\n",
    "                run.font.italic = original_runs[i].font.italic\n",
    "                run.font.color.rgb = original_runs[i].font.color.rgb\n",
    "                run.font.underline = original_runs[i].font.underline\n",
    "                # Add more formatting attributes as needed\n",
    "\n",
    "        # Let us log the corrections made\n",
    "        corrections_doc.add_paragraph()\n",
    "        corrections_doc.add_paragraph(f\"Original Text : \\n {text}\")\n",
    "        corrections_doc.add_paragraph(f\"Edits : \\n {edits}\")\n",
    "        corrections_doc.add_paragraph(f\"Corrections: \\n{llm_output_text}\")\n",
    "\n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "\n",
    "def process_document_tables(modified_doc, corrections_doc):\n",
    "\n",
    "    ## REPLACE this with logic of the modified doc\n",
    "    # modified_doc = deepcopy(input_doc)\n",
    "    # Iterate through all tables in the document\n",
    "    for table in modified_doc.tables:\n",
    "        print(\"IN Table\")\n",
    "        printed_cells = set()  # To keep track of cells that have been processed\n",
    "        for r_index, row in enumerate(table.rows):\n",
    "            for c_index, cell in enumerate(row.cells):\n",
    "                cell_id = (r_index, c_index)  # Unique identifier for the cell\n",
    "\n",
    "                # Skip this cell if it is already processed as part of a merged cell\n",
    "                if cell_id in printed_cells:\n",
    "                    continue\n",
    "\n",
    "                # Detect merged cells\n",
    "                is_merged = False\n",
    "                for other_cell in row.cells:\n",
    "                    if other_cell is not cell and other_cell._element is cell._element:\n",
    "                        is_merged = True\n",
    "                        break\n",
    "\n",
    "                # If it's a merged cell, avoid processing duplicates\n",
    "                if is_merged:\n",
    "                    # Register this cell's element to skip duplicates\n",
    "                    for merged_row_index, merged_row in enumerate(table.rows):\n",
    "                        for merged_cell_index, merged_cell in enumerate(\n",
    "                            merged_row.cells\n",
    "                        ):\n",
    "                            if merged_cell._element is cell._element:\n",
    "                                printed_cells.add((merged_row_index, merged_cell_index))\n",
    "\n",
    "                # Append '**' to the text of the cell if not already processed\n",
    "                if cell.text.strip():  # Check if the cell is not empty\n",
    "                    #                    cell.text += '*T*B*L'\n",
    "                    for para in cell.paragraphs:\n",
    "                        # Add an asterisk (*) to the end of each cell paragraph\n",
    "                        print(para.text)\n",
    "                        # Just a small check to see that we processed this\n",
    "                        # para.add_run('*T')\n",
    "\n",
    "                        # Store the original formatting\n",
    "                        original_runs = para.runs.copy()\n",
    "                        # let us call the llm\n",
    "                        llm_output_text = fetch_llm_response(para.text)\n",
    "\n",
    "                        # edits, corrections = document_postprocessing(llm_output_text)\n",
    "                        edits, corrections = process_llm_output(llm_output_text)\n",
    "\n",
    "                        # let us reapply formatting\n",
    "\n",
    "                        # Clear the paragraph and add the edited text\n",
    "                        para.clear()\n",
    "                        # para.add_run(edited_text['choices'][0]['text'].strip())\n",
    "                        para.add_run(edits)\n",
    "                        # Attempt to reapply formatting\n",
    "                        new_runs = para.runs\n",
    "                        for i, run in enumerate(new_runs):\n",
    "                            if i < len(original_runs):\n",
    "                                run.font.name = original_runs[i].font.name\n",
    "                                run.font.size = original_runs[i].font.size\n",
    "                                run.font.bold = original_runs[i].font.bold\n",
    "                                run.font.italic = original_runs[i].font.italic\n",
    "                                run.font.color.rgb = original_runs[i].font.color.rgb\n",
    "                                run.font.underline = original_runs[i].font.underline\n",
    "                                # Add more formatting attributes as needed\n",
    "\n",
    "                        # Let us log the corrections made\n",
    "                        corrections_doc.add_paragraph()\n",
    "                        corrections_doc.add_paragraph(f\"Original Text : \\n {para.text}\")\n",
    "                        corrections_doc.add_paragraph(\n",
    "                            f\"Corrections: \\n{llm_output_text}\"\n",
    "                        )\n",
    "\n",
    "            print()  # Newline after each row\n",
    "\n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "\n",
    "def proofread_and_correct_document(modified_doc, corrections_doc):\n",
    "    corrections_doc.add_heading(\"Corrections Made\", 0)\n",
    "\n",
    "    # Correct using deterministic grammar checking\n",
    "    modified_doc, corrections_doc = pre_process_document(modified_doc, corrections_doc)\n",
    "    modified_doc, corrections_doc = process_document_paragraphs(\n",
    "        modified_doc, corrections_doc\n",
    "    )\n",
    "    modified_doc, corrections_doc = process_document_tables(\n",
    "        modified_doc, corrections_doc\n",
    "    )\n",
    "\n",
    "    return modified_doc, corrections_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1268 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18472.61 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    28 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  441594.36 ms /    47 tokens\n",
      "Llama.generate: 1268 prefix-match hit, remaining 10 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " Use the following examples as reference for editing.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 1.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I saw a    sample prodcut.  These product are below expeccted quality. I start to underdstand what he said is quite right. The correct valu is 12.5%% and 3rd postion. The time is 10:30am. The length of the object is 845mm and costs $1million. The sunise happens at 0750 EST and visibilty is 35 km. The right way too sumize and favor some advisor's is still to be found.'\n",
      "\n",
      "1. Edited Text: 'I saw a sample product. These products are below expected quality. I started to understand what he said is quite right. The correct value is 12.5%% and 3rd position. The time is 10:30 am. The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibility is 35 km. The right way to summarize and favor some advisors' is still to be found.'\n",
      "2. Corrections:\n",
      "   a) Removed extra spaces after 'I saw' and before 'These products are below expected quality.'\n",
      "   b) Changed 'sample prodcut' to 'sample product.'\n",
      "   c) Corrected the tense from 'start' to 'started'\n",
      "   d) Corrected the spelling of 'expeccted quality.' to 'expected quality.'\n",
      "   e) Fixed the spelling \"correct valu\" to 'correct value.'\n",
      "   f) Fixed the time format to '10:30 am.'\n",
      "   g) Abbreviated '$1million' to '$1m.'\n",
      "   h) Corrected 'sunise' to 'sunrise.'\n",
      "   i) Fixed the spelling of'visibilty' to 'visibility.'\n",
      "   j) Corrected 'too sumize' to 'to summarize.'\n",
      "   k) Fixed punctuation in 'advisor's' to 'advisors'.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 2.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I read a article about new technology. These technologie are not very impresssive. I think the points he made are mostly valid. The value should be 15.3%% and in 2nd postion. The time is 9:45am. The object's width is 732cm and priced at $500,000. The moonrise occured at 0635 UTC and the visibility range is 40 miles. The correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "\n",
      "1. Edited Text: 'I read an article about new technology. These technologies are not very impressive. I think the points he made are mostly valid. The value should be 15.3%% and in 2nd position. The time is 9:45 am. The object's width is 732cm and priced at $500,000. The moonrise occurred at 0635 UTC and the visibility range is 40 miles. The correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "2. Corrections: \n",
      "   a) Changed 'I read a article' to 'I read an article.' \n",
      "   b) Corrected spelling of 'technologie' to 'technologies.' \n",
      "   c) Fixed spelling of 'impresssive' to 'impressive.' \n",
      "   d) Corrected spelling of 'postion' to 'position.' \n",
      "   e) Changed the time format to '9:45 am.'  \n",
      "   f) Updated 'priced at $500,000' for clarity and consistency. \n",
      "   g) Corrected spelling of 'occured' to 'occurred.' \n",
      "   h) Replaced 'visibility is 40 miles' to 'visibility range is 40 miles.' \n",
      "   i) Clarified 'correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 3.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I have a book called \"The Sun Also Rises\". The book is about a group of American and British expatriates who travel from Paris to Pamplona to watch the running of the bulls and the bullfights. The book was published in 1926 and was written by Ernest Hemingway. The main characters are Jake Barnes, Lady Brett Ashley, Robert Cohn, and Pedro Romero. The book is considered one of Hemingway's masterpieces and is a classic of American literature.'\n",
      "\n",
      "\n",
      "1. Edited Text: \n",
      "\n",
      "2. Corrections:\n",
      "\n",
      "------------------------------------------------------------\n",
      "Following is the text you have to edit: \n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: ' Ref No: MC24-111111'\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: \n",
      "   Ref No: MC24-111111\n",
      "2. Corrections: \n",
      "   None\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Ref No: MC24-111111\n",
      "\n",
      "Corrections:\n",
      "None\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Ref No: MC24-111111\n",
      "CORRECTIONS: \n",
      " None\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   18472.61 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  273642.42 ms /    27 tokens\n",
      "Llama.generate: 1268 prefix-match hit, remaining 8 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " Use the following examples as reference for editing.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 1.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I saw a    sample prodcut.  These product are below expeccted quality. I start to underdstand what he said is quite right. The correct valu is 12.5%% and 3rd postion. The time is 10:30am. The length of the object is 845mm and costs $1million. The sunise happens at 0750 EST and visibilty is 35 km. The right way too sumize and favor some advisor's is still to be found.'\n",
      "\n",
      "1. Edited Text: 'I saw a sample product. These products are below expected quality. I started to understand what he said is quite right. The correct value is 12.5%% and 3rd position. The time is 10:30 am. The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibility is 35 km. The right way to summarize and favor some advisors' is still to be found.'\n",
      "2. Corrections:\n",
      "   a) Removed extra spaces after 'I saw' and before 'These products are below expected quality.'\n",
      "   b) Changed 'sample prodcut' to 'sample product.'\n",
      "   c) Corrected the tense from 'start' to 'started'\n",
      "   d) Corrected the spelling of 'expeccted quality.' to 'expected quality.'\n",
      "   e) Fixed the spelling \"correct valu\" to 'correct value.'\n",
      "   f) Fixed the time format to '10:30 am.'\n",
      "   g) Abbreviated '$1million' to '$1m.'\n",
      "   h) Corrected 'sunise' to 'sunrise.'\n",
      "   i) Fixed the spelling of'visibilty' to 'visibility.'\n",
      "   j) Corrected 'too sumize' to 'to summarize.'\n",
      "   k) Fixed punctuation in 'advisor's' to 'advisors'.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 2.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I read a article about new technology. These technologie are not very impresssive. I think the points he made are mostly valid. The value should be 15.3%% and in 2nd postion. The time is 9:45am. The object's width is 732cm and priced at $500,000. The moonrise occured at 0635 UTC and the visibility range is 40 miles. The correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "\n",
      "1. Edited Text: 'I read an article about new technology. These technologies are not very impressive. I think the points he made are mostly valid. The value should be 15.3%% and in 2nd position. The time is 9:45 am. The object's width is 732cm and priced at $500,000. The moonrise occurred at 0635 UTC and the visibility range is 40 miles. The correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "2. Corrections: \n",
      "   a) Changed 'I read a article' to 'I read an article.' \n",
      "   b) Corrected spelling of 'technologie' to 'technologies.' \n",
      "   c) Fixed spelling of 'impresssive' to 'impressive.' \n",
      "   d) Corrected spelling of 'postion' to 'position.' \n",
      "   e) Changed the time format to '9:45 am.'  \n",
      "   f) Updated 'priced at $500,000' for clarity and consistency. \n",
      "   g) Corrected spelling of 'occured' to 'occurred.' \n",
      "   h) Replaced 'visibility is 40 miles' to 'visibility range is 40 miles.' \n",
      "   i) Clarified 'correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 3.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I have a book called \"The Sun Also Rises\". The book is about a group of American and British expatriates who travel from Paris to Pamplona to watch the running of the bulls and the bullfights. The book was published in 1926 and was written by Ernest Hemingway. The main characters are Jake Barnes, Lady Brett Ashley, Robert Cohn, and Pedro Romero. The book is considered one of Hemingway's masterpieces and is a classic of American literature.'\n",
      "\n",
      "\n",
      "1. Edited Text: \n",
      "\n",
      "2. Corrections:\n",
      "\n",
      "------------------------------------------------------------\n",
      "Following is the text you have to edit: \n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: 'Mr Ken Bearers'\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: \n",
      "'Mr Ken Bearers'\n",
      "2. Corrections: None\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Mr Ken Bearers\n",
      "\n",
      "Corrections:\n",
      "None\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Mr Ken Bearers\n",
      "CORRECTIONS: \n",
      " None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   18472.61 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  243692.71 ms /    23 tokens\n",
      "Llama.generate: 1268 prefix-match hit, remaining 12 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " Use the following examples as reference for editing.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 1.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I saw a    sample prodcut.  These product are below expeccted quality. I start to underdstand what he said is quite right. The correct valu is 12.5%% and 3rd postion. The time is 10:30am. The length of the object is 845mm and costs $1million. The sunise happens at 0750 EST and visibilty is 35 km. The right way too sumize and favor some advisor's is still to be found.'\n",
      "\n",
      "1. Edited Text: 'I saw a sample product. These products are below expected quality. I started to understand what he said is quite right. The correct value is 12.5%% and 3rd position. The time is 10:30 am. The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibility is 35 km. The right way to summarize and favor some advisors' is still to be found.'\n",
      "2. Corrections:\n",
      "   a) Removed extra spaces after 'I saw' and before 'These products are below expected quality.'\n",
      "   b) Changed 'sample prodcut' to 'sample product.'\n",
      "   c) Corrected the tense from 'start' to 'started'\n",
      "   d) Corrected the spelling of 'expeccted quality.' to 'expected quality.'\n",
      "   e) Fixed the spelling \"correct valu\" to 'correct value.'\n",
      "   f) Fixed the time format to '10:30 am.'\n",
      "   g) Abbreviated '$1million' to '$1m.'\n",
      "   h) Corrected 'sunise' to 'sunrise.'\n",
      "   i) Fixed the spelling of'visibilty' to 'visibility.'\n",
      "   j) Corrected 'too sumize' to 'to summarize.'\n",
      "   k) Fixed punctuation in 'advisor's' to 'advisors'.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 2.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I read a article about new technology. These technologie are not very impresssive. I think the points he made are mostly valid. The value should be 15.3%% and in 2nd postion. The time is 9:45am. The object's width is 732cm and priced at $500,000. The moonrise occured at 0635 UTC and the visibility range is 40 miles. The correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "\n",
      "1. Edited Text: 'I read an article about new technology. These technologies are not very impressive. I think the points he made are mostly valid. The value should be 15.3%% and in 2nd position. The time is 9:45 am. The object's width is 732cm and priced at $500,000. The moonrise occurred at 0635 UTC and the visibility range is 40 miles. The correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "2. Corrections: \n",
      "   a) Changed 'I read a article' to 'I read an article.' \n",
      "   b) Corrected spelling of 'technologie' to 'technologies.' \n",
      "   c) Fixed spelling of 'impresssive' to 'impressive.' \n",
      "   d) Corrected spelling of 'postion' to 'position.' \n",
      "   e) Changed the time format to '9:45 am.'  \n",
      "   f) Updated 'priced at $500,000' for clarity and consistency. \n",
      "   g) Corrected spelling of 'occured' to 'occurred.' \n",
      "   h) Replaced 'visibility is 40 miles' to 'visibility range is 40 miles.' \n",
      "   i) Clarified 'correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 3.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I have a book called \"The Sun Also Rises\". The book is about a group of American and British expatriates who travel from Paris to Pamplona to watch the running of the bulls and the bullfights. The book was published in 1926 and was written by Ernest Hemingway. The main characters are Jake Barnes, Lady Brett Ashley, Robert Cohn, and Pedro Romero. The book is considered one of Hemingway's masterpieces and is a classic of American literature.'\n",
      "\n",
      "\n",
      "1. Edited Text: \n",
      "\n",
      "2. Corrections:\n",
      "\n",
      "------------------------------------------------------------\n",
      "Following is the text you have to edit: \n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: 'Branch President'\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: \n",
      "Branch President\n",
      "2. Corrections: \n",
      "None\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Branch President\n",
      "\n",
      "Corrections:\n",
      "None\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Branch President\n",
      "CORRECTIONS: \n",
      " None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   18472.61 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  289334.48 ms /    30 tokens\n",
      "Llama.generate: 1268 prefix-match hit, remaining 17 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " Use the following examples as reference for editing.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 1.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I saw a    sample prodcut.  These product are below expeccted quality. I start to underdstand what he said is quite right. The correct valu is 12.5%% and 3rd postion. The time is 10:30am. The length of the object is 845mm and costs $1million. The sunise happens at 0750 EST and visibilty is 35 km. The right way too sumize and favor some advisor's is still to be found.'\n",
      "\n",
      "1. Edited Text: 'I saw a sample product. These products are below expected quality. I started to understand what he said is quite right. The correct value is 12.5%% and 3rd position. The time is 10:30 am. The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibility is 35 km. The right way to summarize and favor some advisors' is still to be found.'\n",
      "2. Corrections:\n",
      "   a) Removed extra spaces after 'I saw' and before 'These products are below expected quality.'\n",
      "   b) Changed 'sample prodcut' to 'sample product.'\n",
      "   c) Corrected the tense from 'start' to 'started'\n",
      "   d) Corrected the spelling of 'expeccted quality.' to 'expected quality.'\n",
      "   e) Fixed the spelling \"correct valu\" to 'correct value.'\n",
      "   f) Fixed the time format to '10:30 am.'\n",
      "   g) Abbreviated '$1million' to '$1m.'\n",
      "   h) Corrected 'sunise' to 'sunrise.'\n",
      "   i) Fixed the spelling of'visibilty' to 'visibility.'\n",
      "   j) Corrected 'too sumize' to 'to summarize.'\n",
      "   k) Fixed punctuation in 'advisor's' to 'advisors'.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 2.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I read a article about new technology. These technologie are not very impresssive. I think the points he made are mostly valid. The value should be 15.3%% and in 2nd postion. The time is 9:45am. The object's width is 732cm and priced at $500,000. The moonrise occured at 0635 UTC and the visibility range is 40 miles. The correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "\n",
      "1. Edited Text: 'I read an article about new technology. These technologies are not very impressive. I think the points he made are mostly valid. The value should be 15.3%% and in 2nd position. The time is 9:45 am. The object's width is 732cm and priced at $500,000. The moonrise occurred at 0635 UTC and the visibility range is 40 miles. The correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "2. Corrections: \n",
      "   a) Changed 'I read a article' to 'I read an article.' \n",
      "   b) Corrected spelling of 'technologie' to 'technologies.' \n",
      "   c) Fixed spelling of 'impresssive' to 'impressive.' \n",
      "   d) Corrected spelling of 'postion' to 'position.' \n",
      "   e) Changed the time format to '9:45 am.'  \n",
      "   f) Updated 'priced at $500,000' for clarity and consistency. \n",
      "   g) Corrected spelling of 'occured' to 'occurred.' \n",
      "   h) Replaced 'visibility is 40 miles' to 'visibility range is 40 miles.' \n",
      "   i) Clarified 'correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 3.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I have a book called \"The Sun Also Rises\". The book is about a group of American and British expatriates who travel from Paris to Pamplona to watch the running of the bulls and the bullfights. The book was published in 1926 and was written by Ernest Hemingway. The main characters are Jake Barnes, Lady Brett Ashley, Robert Cohn, and Pedro Romero. The book is considered one of Hemingway's masterpieces and is a classic of American literature.'\n",
      "\n",
      "\n",
      "1. Edited Text: \n",
      "\n",
      "2. Corrections:\n",
      "\n",
      "------------------------------------------------------------\n",
      "Following is the text you have to edit: \n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: 'Gainsborough Neighbours Party'\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: \n",
      "Gainsborough Neighbours Party\n",
      "2. Corrections: None\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Gainsborough Neighbours Party\n",
      "\n",
      "Corrections:\n",
      "None\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Gainsborough Neighbours Party\n",
      "CORRECTIONS: \n",
      " None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   18472.61 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  351330.92 ms /    39 tokens\n",
      "Llama.generate: 1268 prefix-match hit, remaining 9 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " Use the following examples as reference for editing.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 1.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I saw a    sample prodcut.  These product are below expeccted quality. I start to underdstand what he said is quite right. The correct valu is 12.5%% and 3rd postion. The time is 10:30am. The length of the object is 845mm and costs $1million. The sunise happens at 0750 EST and visibilty is 35 km. The right way too sumize and favor some advisor's is still to be found.'\n",
      "\n",
      "1. Edited Text: 'I saw a sample product. These products are below expected quality. I started to understand what he said is quite right. The correct value is 12.5%% and 3rd position. The time is 10:30 am. The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibility is 35 km. The right way to summarize and favor some advisors' is still to be found.'\n",
      "2. Corrections:\n",
      "   a) Removed extra spaces after 'I saw' and before 'These products are below expected quality.'\n",
      "   b) Changed 'sample prodcut' to 'sample product.'\n",
      "   c) Corrected the tense from 'start' to 'started'\n",
      "   d) Corrected the spelling of 'expeccted quality.' to 'expected quality.'\n",
      "   e) Fixed the spelling \"correct valu\" to 'correct value.'\n",
      "   f) Fixed the time format to '10:30 am.'\n",
      "   g) Abbreviated '$1million' to '$1m.'\n",
      "   h) Corrected 'sunise' to 'sunrise.'\n",
      "   i) Fixed the spelling of'visibilty' to 'visibility.'\n",
      "   j) Corrected 'too sumize' to 'to summarize.'\n",
      "   k) Fixed punctuation in 'advisor's' to 'advisors'.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 2.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I read a article about new technology. These technologie are not very impresssive. I think the points he made are mostly valid. The value should be 15.3%% and in 2nd postion. The time is 9:45am. The object's width is 732cm and priced at $500,000. The moonrise occured at 0635 UTC and the visibility range is 40 miles. The correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "\n",
      "1. Edited Text: 'I read an article about new technology. These technologies are not very impressive. I think the points he made are mostly valid. The value should be 15.3%% and in 2nd position. The time is 9:45 am. The object's width is 732cm and priced at $500,000. The moonrise occurred at 0635 UTC and the visibility range is 40 miles. The correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "2. Corrections: \n",
      "   a) Changed 'I read a article' to 'I read an article.' \n",
      "   b) Corrected spelling of 'technologie' to 'technologies.' \n",
      "   c) Fixed spelling of 'impresssive' to 'impressive.' \n",
      "   d) Corrected spelling of 'postion' to 'position.' \n",
      "   e) Changed the time format to '9:45 am.'  \n",
      "   f) Updated 'priced at $500,000' for clarity and consistency. \n",
      "   g) Corrected spelling of 'occured' to 'occurred.' \n",
      "   h) Replaced 'visibility is 40 miles' to 'visibility range is 40 miles.' \n",
      "   i) Clarified 'correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 3.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I have a book called \"The Sun Also Rises\". The book is about a group of American and British expatriates who travel from Paris to Pamplona to watch the running of the bulls and the bullfights. The book was published in 1926 and was written by Ernest Hemingway. The main characters are Jake Barnes, Lady Brett Ashley, Robert Cohn, and Pedro Romero. The book is considered one of Hemingway's masterpieces and is a classic of American literature.'\n",
      "\n",
      "\n",
      "1. Edited Text: \n",
      "\n",
      "2. Corrections:\n",
      "\n",
      "------------------------------------------------------------\n",
      "Following is the text you have to edit: \n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: 'erinsborough.neighbours.branch@fakemail.com '\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: \n",
      "erinsborough.neighbours.branch@fakemail.com\n",
      "2. Corrections:\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "erinsborough.neighbours.branch@fakemail.com\n",
      "\n",
      "Corrections:\n",
      "\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " erinsborough.neighbours.branch@fakemail.com\n",
      "CORRECTIONS: \n",
      " \n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   18472.61 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  244666.07 ms /    24 tokens\n",
      "Llama.generate: 1268 prefix-match hit, remaining 56 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " Use the following examples as reference for editing.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 1.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I saw a    sample prodcut.  These product are below expeccted quality. I start to underdstand what he said is quite right. The correct valu is 12.5%% and 3rd postion. The time is 10:30am. The length of the object is 845mm and costs $1million. The sunise happens at 0750 EST and visibilty is 35 km. The right way too sumize and favor some advisor's is still to be found.'\n",
      "\n",
      "1. Edited Text: 'I saw a sample product. These products are below expected quality. I started to understand what he said is quite right. The correct value is 12.5%% and 3rd position. The time is 10:30 am. The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibility is 35 km. The right way to summarize and favor some advisors' is still to be found.'\n",
      "2. Corrections:\n",
      "   a) Removed extra spaces after 'I saw' and before 'These products are below expected quality.'\n",
      "   b) Changed 'sample prodcut' to 'sample product.'\n",
      "   c) Corrected the tense from 'start' to 'started'\n",
      "   d) Corrected the spelling of 'expeccted quality.' to 'expected quality.'\n",
      "   e) Fixed the spelling \"correct valu\" to 'correct value.'\n",
      "   f) Fixed the time format to '10:30 am.'\n",
      "   g) Abbreviated '$1million' to '$1m.'\n",
      "   h) Corrected 'sunise' to 'sunrise.'\n",
      "   i) Fixed the spelling of'visibilty' to 'visibility.'\n",
      "   j) Corrected 'too sumize' to 'to summarize.'\n",
      "   k) Fixed punctuation in 'advisor's' to 'advisors'.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 2.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I read a article about new technology. These technologie are not very impresssive. I think the points he made are mostly valid. The value should be 15.3%% and in 2nd postion. The time is 9:45am. The object's width is 732cm and priced at $500,000. The moonrise occured at 0635 UTC and the visibility range is 40 miles. The correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "\n",
      "1. Edited Text: 'I read an article about new technology. These technologies are not very impressive. I think the points he made are mostly valid. The value should be 15.3%% and in 2nd position. The time is 9:45 am. The object's width is 732cm and priced at $500,000. The moonrise occurred at 0635 UTC and the visibility range is 40 miles. The correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "2. Corrections: \n",
      "   a) Changed 'I read a article' to 'I read an article.' \n",
      "   b) Corrected spelling of 'technologie' to 'technologies.' \n",
      "   c) Fixed spelling of 'impresssive' to 'impressive.' \n",
      "   d) Corrected spelling of 'postion' to 'position.' \n",
      "   e) Changed the time format to '9:45 am.'  \n",
      "   f) Updated 'priced at $500,000' for clarity and consistency. \n",
      "   g) Corrected spelling of 'occured' to 'occurred.' \n",
      "   h) Replaced 'visibility is 40 miles' to 'visibility range is 40 miles.' \n",
      "   i) Clarified 'correct strategy for optimizing results and consulting a specialist's insights remains unclear.'\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 3.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: \n",
      "'I have a book called \"The Sun Also Rises\". The book is about a group of American and British expatriates who travel from Paris to Pamplona to watch the running of the bulls and the bullfights. The book was published in 1926 and was written by Ernest Hemingway. The main characters are Jake Barnes, Lady Brett Ashley, Robert Cohn, and Pedro Romero. The book is considered one of Hemingway's masterpieces and is a classic of American literature.'\n",
      "\n",
      "\n",
      "1. Edited Text: \n",
      "\n",
      "2. Corrections:\n",
      "\n",
      "------------------------------------------------------------\n",
      "Following is the text you have to edit: \n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: 'Dear Mr Ken'\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Dear Mr Ken'\n",
      "2. Corrections: None\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Dear Mr Ken\n",
      "\n",
      "Corrections:\n",
      "None\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Dear Mr Ken\n",
      "CORRECTIONS: \n",
      " None\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a document object for corrections\u001b[39;00m\n\u001b[1;32m      7\u001b[0m corrections_doc \u001b[38;5;241m=\u001b[39m Document()\n\u001b[0;32m----> 8\u001b[0m modified_doc, corrections_doc \u001b[38;5;241m=\u001b[39m \u001b[43mproofread_and_correct_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodified_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrections_doc\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m modified_doc\u001b[38;5;241m.\u001b[39msave(edit_path)\n\u001b[1;32m     12\u001b[0m corrections_doc\u001b[38;5;241m.\u001b[39msave(correction_path)\n",
      "Cell \u001b[0;32mIn[1], line 270\u001b[0m, in \u001b[0;36mproofread_and_correct_document\u001b[0;34m(modified_doc, corrections_doc)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Correct using deterministic grammar checking\u001b[39;00m\n\u001b[1;32m    269\u001b[0m modified_doc, corrections_doc \u001b[38;5;241m=\u001b[39m pre_process_document(modified_doc, corrections_doc)\n\u001b[0;32m--> 270\u001b[0m modified_doc, corrections_doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_document_paragraphs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodified_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrections_doc\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m modified_doc, corrections_doc \u001b[38;5;241m=\u001b[39m process_document_tables(\n\u001b[1;32m    274\u001b[0m     modified_doc, corrections_doc\n\u001b[1;32m    275\u001b[0m )\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m modified_doc, corrections_doc\n",
      "Cell \u001b[0;32mIn[1], line 154\u001b[0m, in \u001b[0;36mprocess_document_paragraphs\u001b[0;34m(modified_doc, corrections_doc)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# Get the text content\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     text \u001b[38;5;241m=\u001b[39m para\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m--> 154\u001b[0m llm_output_text \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# edits, corrections = document_postprocessing(llm_output_text)\u001b[39;00m\n\u001b[1;32m    157\u001b[0m edits, corrections \u001b[38;5;241m=\u001b[39m process_llm_output(llm_output_text)\n",
      "Cell \u001b[0;32mIn[1], line 107\u001b[0m, in \u001b[0;36mfetch_llm_response\u001b[0;34m(text, prompt_template, sys_prompt)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     98\u001b[0m         {\n\u001b[1;32m     99\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m         },\n\u001b[1;32m    106\u001b[0m     ]\n\u001b[0;32m--> 107\u001b[0m edited_text \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m response_text \u001b[38;5;241m=\u001b[39m edited_text[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROMPT: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_template\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mtext)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_cpp/llama.py:1999\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \n\u001b[1;32m   1963\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1994\u001b[0m handler \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1995\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   1997\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_completion_handler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   1998\u001b[0m )\n\u001b[0;32m-> 1999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_cpp/llama_chat_format.py:637\u001b[0m, in \u001b[0;36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e), file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    633\u001b[0m         grammar \u001b[38;5;241m=\u001b[39m llama_grammar\u001b[38;5;241m.\u001b[39mLlamaGrammar\u001b[38;5;241m.\u001b[39mfrom_string(\n\u001b[1;32m    634\u001b[0m             llama_grammar\u001b[38;5;241m.\u001b[39mJSON_GBNF, verbose\u001b[38;5;241m=\u001b[39mllama\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    635\u001b[0m         )\n\u001b[0;32m--> 637\u001b[0m completion_or_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    663\u001b[0m     tool_name \u001b[38;5;241m=\u001b[39m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_cpp/llama.py:1833\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1831\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1833\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_cpp/llama.py:1318\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1316\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1317\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1318\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_token_is_eog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_cpp/llama.py:910\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 910\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    912\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    913\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    914\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    929\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_cpp/llama.py:643\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    639\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    641\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    642\u001b[0m )\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_cpp/_internals.py:300\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[0;32m--> 300\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_path = \"/home/cdsw/data/input.docx\"\n",
    "edit_path = \"/home/cdsw/data/output_doc.docx\"\n",
    "correction_path = \"/home/cdsw/data/correction_file.docx\"\n",
    "input_doc = Document(input_path)\n",
    "modified_doc = deepcopy(input_doc)\n",
    "# Create a document object for corrections\n",
    "corrections_doc = Document()\n",
    "modified_doc, corrections_doc = proofread_and_correct_document(\n",
    "    modified_doc, corrections_doc\n",
    ")\n",
    "modified_doc.save(edit_path)\n",
    "corrections_doc.save(correction_path)\n",
    "\n",
    "\n",
    "input_doc = Document(input_path)\n",
    "# make a deep copy because Redline removes the input document file\n",
    "redline_input_doc = deepcopy(input_doc)\n",
    "redline_input_doc_path=\"/home/cdsw/data/redline_input.docx\"\n",
    "redline_input_doc.save(redline_input_doc_path)\n",
    "\n",
    "from python_redlines.engines import XmlPowerToolsEngine\n",
    "wrapper = XmlPowerToolsEngine()\n",
    "\n",
    "output_redlines = wrapper.run_redline('Smartdoc Processor', redline_input_doc_path, edit_path)\n",
    "with open('/home/cdsw/data/redline_output.docx', 'wb') as f:\n",
    "    f.write(output_redlines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
