{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361aa329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -q open-webui python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab8a9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/cdsw/.cache/huggingface/hub/models--TheBloke--Karen_TheEditor_V2_STRICT_Mistral_7B-GGUF/snapshots/6c654aa207a4b673379db7a928b87a01d644676c/karen_theeditor_v2_strict_mistral_7b.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = fpham_karen_theeditor_v2_strict_mistr...\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = fpham_karen_theeditor_v2_strict_mistral_7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7338.66 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 16384\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  1088.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'fpham_karen_theeditor_v2_strict_mistral_7b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "#imports and set up the local model\n",
    "from docx import Document\n",
    "from transformers import pipeline\n",
    "from llama_cpp import Llama\n",
    "from copy import deepcopy\n",
    "from smartdoc_utils import document_postprocessing, process_llm_output\n",
    "model_path = \"/home/cdsw/.cache/huggingface/hub/models--TheBloke--Karen_TheEditor_V2_STRICT_Mistral_7B-GGUF/snapshots/6c654aa207a4b673379db7a928b87a01d644676c/karen_theeditor_v2_strict_mistral_7b.Q8_0.gguf\"\n",
    "# Initialize the model with GPU support\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,  # -1 means use all available GPU layers\n",
    "    n_ctx=16384,  # adjust based on your GPU memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f0b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_llm_response(text):\n",
    "    #Use the LLM to proofread and edit\n",
    "    # prompt = (\n",
    "    #     \"<|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\\n\"\n",
    "    #     \"<|user|> ** TASK**  \\n\" \n",
    "    #     f\"1. Edit the following text for spelling and grammar mistakes:  '{text}'\\n\"\n",
    "    #     f\"2. Remove any formatting errors such as extra spaces \\n\"\n",
    "    #     f\"**IMPORTANT** use the following template to format your response **.\\n\"\n",
    "    #     f\"1. Edited Text: \\n\"\n",
    "    #     f\"[ Your corrected text here ]\\n\"\n",
    "    #     f\"2. Corrections: \\n\"\n",
    "    #     f\"[ list **each correction made here** ]\\n\"\n",
    "    #     \"<|end|>\\n\"\n",
    "    #     \"<|assistant|>\\n\"\n",
    "    # )\n",
    "    prompt = (\n",
    "        \"<|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\\n\"\n",
    "        \"<|user|> ** TASK**  \\n\" \n",
    "        f\"1. Edit the following text for spelling and grammar mistakes:  '{text}'\\n\"\n",
    "        f\"2. Remove any formatting errors such as extra spaces\"\n",
    "        f\"**IMPORTANT** use the following template to format your response **.\"\n",
    "        f\"1. Edited Text: \"\n",
    "        f\"[ Your corrected text here ]\"\n",
    "        f\"2. Corrections: \"\n",
    "        f\"[ Make a numbered list of your corrections ]\"\n",
    "        \"<|end|>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "    )    \n",
    "#        edited_text = llm(prompt, max_length=len(text) + 50)[0]['generated_text']\n",
    "    edited_text=llm(prompt, max_tokens=400, temperature=0.7, top_p=0.1, top_k=40, repeat_penalty=1.18)\n",
    "    response_text = edited_text['choices'][0]['text'].strip()\n",
    "    print(f\"PROMPT: \\n {prompt} \\n\\n\")\n",
    "    print(f\"LLM RESPONSE: \\n{response_text}\\n\\n\")      \n",
    "    \n",
    "    return response_text\n",
    "    \n",
    "# code that Aamir showed with language_tool checks comes here\n",
    "# Pre-process an input document to handle formatting, \n",
    "# language checks, etc. before further processing\n",
    "#\n",
    "# Args:\n",
    "#   input_doc: The original input document \n",
    "#   corrections_doc: An empty document to populate with corrections \n",
    "#\n",
    "# Returns: \n",
    "#   modified_doc: A modified version of the input document \n",
    "#   corrections_doc: Populated with any corrections made during p\n",
    "def pre_process_document(input_doc, corrections_doc): \n",
    "    \n",
    "    ## REPLACE this with logic of the modified doc\n",
    "    modified_doc = deepcopy(input_doc)\n",
    "    \n",
    "    # logic for pre-processing Fonts, Australian Language Checks, etc. \n",
    "    \n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "def process_document_paragraphs(modified_doc, corrections_doc) : \n",
    "\n",
    "    #modified_doc = deepcopy(input_doc)\n",
    "    # Process each paragraph\n",
    "    for para in modified_doc.paragraphs:\n",
    "        # Store the original formatting\n",
    "        original_runs = para.runs.copy()\n",
    "        \n",
    "        if not para.text.strip():\n",
    "            print(\"Skipping LLM CALL:\")\n",
    "            continue\n",
    "        else:\n",
    "            # Get the text content\n",
    "            text = para.text\n",
    "\n",
    "        llm_output_text = fetch_llm_response(text)\n",
    "\n",
    "        # edits, corrections = document_postprocessing(llm_output_text)\n",
    "        edits, corrections = process_llm_output(llm_output_text)\n",
    "        print(\"EDITS : \\n\", edits)\n",
    "        print(\"CORRECTIONS: \\n\", corrections)\n",
    "        # Clear the paragraph and add the edited text\n",
    "        para.clear()\n",
    "        #para.add_run(edited_text['choices'][0]['text'].strip())\n",
    "        para.add_run(edits)\n",
    "        # Attempt to reapply formatting\n",
    "        new_runs = para.runs\n",
    "        for i, run in enumerate(new_runs):\n",
    "            if i < len(original_runs):\n",
    "                run.font.name = original_runs[i].font.name\n",
    "                run.font.size = original_runs[i].font.size\n",
    "                run.font.bold = original_runs[i].font.bold\n",
    "                run.font.italic = original_runs[i].font.italic\n",
    "                run.font.color.rgb = original_runs[i].font.color.rgb\n",
    "                run.font.underline = original_runs[i].font.underline    \n",
    "                # Add more formatting attributes as needed\n",
    "        \n",
    "\n",
    "        # Let us log the corrections made\n",
    "        corrections_doc.add_paragraph()\n",
    "        corrections_doc.add_paragraph(f\"Original Text : \\n {text}\")\n",
    "        corrections_doc.add_paragraph(f\"Edits : \\n {edits}\")\n",
    "        corrections_doc.add_paragraph(f\"Corrections: \\n{llm_output_text}\")\n",
    "                   \n",
    "    \n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "    \n",
    "def process_document_tables(modified_doc, corrections_doc) : \n",
    "    \n",
    "    ## REPLACE this with logic of the modified doc\n",
    "    #modified_doc = deepcopy(input_doc)\n",
    "    # Iterate through all tables in the document\n",
    "    for table in modified_doc.tables:\n",
    "        print(\"IN Table\")        \n",
    "        printed_cells = set()  # To keep track of cells that have been processed\n",
    "        for r_index, row in enumerate(table.rows):\n",
    "            for c_index, cell in enumerate(row.cells):\n",
    "                cell_id = (r_index, c_index)  # Unique identifier for the cell\n",
    "                \n",
    "                # Skip this cell if it is already processed as part of a merged cell\n",
    "                if cell_id in printed_cells:\n",
    "                    continue\n",
    "\n",
    "                # Detect merged cells\n",
    "                is_merged = False\n",
    "                for other_cell in row.cells:\n",
    "                    if other_cell is not cell and other_cell._element is cell._element:\n",
    "                        is_merged = True\n",
    "                        break\n",
    "\n",
    "                # If it's a merged cell, avoid processing duplicates\n",
    "                if is_merged:\n",
    "                    # Register this cell's element to skip duplicates\n",
    "                    for merged_row_index, merged_row in enumerate(table.rows):\n",
    "                        for merged_cell_index, merged_cell in enumerate(merged_row.cells):\n",
    "                            if merged_cell._element is cell._element:\n",
    "                                printed_cells.add((merged_row_index, merged_cell_index))\n",
    "\n",
    "                # Append '**' to the text of the cell if not already processed\n",
    "                if cell.text.strip():  # Check if the cell is not empty\n",
    "#                    cell.text += '*T*B*L'\n",
    "                    for para in cell.paragraphs:\n",
    "                        # Add an asterisk (*) to the end of each cell paragraph\n",
    "                        print(para.text)\n",
    "                        # Just a small check to see that we processed this\n",
    "                        #para.add_run('*T')\n",
    "\n",
    "                        # Store the original formatting\n",
    "                        original_runs = para.runs.copy()                        \n",
    "                        # let us call the llm \n",
    "                        llm_output_text = fetch_llm_response(para.text)\n",
    "\n",
    "                        # edits, corrections = document_postprocessing(llm_output_text)\n",
    "                        edits, corrections = process_llm_output(llm_output_text) \n",
    "                        \n",
    "                        #let us reapply formatting\n",
    "                        \n",
    "                        # Clear the paragraph and add the edited text\n",
    "                        para.clear()\n",
    "                        #para.add_run(edited_text['choices'][0]['text'].strip())\n",
    "                        para.add_run(edits)\n",
    "                        # Attempt to reapply formatting\n",
    "                        new_runs = para.runs\n",
    "                        for i, run in enumerate(new_runs):\n",
    "                            if i < len(original_runs):\n",
    "                                run.font.name = original_runs[i].font.name\n",
    "                                run.font.size = original_runs[i].font.size\n",
    "                                run.font.bold = original_runs[i].font.bold\n",
    "                                run.font.italic = original_runs[i].font.italic\n",
    "                                run.font.color.rgb = original_runs[i].font.color.rgb \n",
    "                                run.font.underline = original_runs[i].font.underline                                                                   \n",
    "                                # Add more formatting attributes as needed                                               \n",
    "\n",
    "                        # Let us log the corrections made\n",
    "                        corrections_doc.add_paragraph()\n",
    "                        corrections_doc.add_paragraph(f\"Original Text : \\n {para.text}\")\n",
    "                        corrections_doc.add_paragraph(f\"Corrections: \\n{llm_output_text}\")\n",
    "                        \n",
    "            print()  # Newline after each row\n",
    "    \n",
    "    \n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "def proofread_document(input_file, output_file, correction_file):\n",
    "    # Load the document\n",
    "    input_doc = Document(input_file)\n",
    "    corrections_doc = Document()\n",
    "    corrections_doc.add_heading(\"Corrections Made\", 0)\n",
    "    modified_doc = deepcopy(input_doc)\n",
    "    modified_doc, corrections_doc = process_document_paragraphs(modified_doc, corrections_doc)\n",
    "    modified_doc, corrections_doc = process_document_tables(modified_doc, corrections_doc)\n",
    "    # Save the edited document\n",
    "    modified_doc.save(output_file)\n",
    "    corrections_doc.save(correction_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99e35ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 57 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      25.77 ms /    39 runs   (    0.66 ms per token,  1513.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2315.10 ms /    75 tokens (   30.87 ms per token,    32.40 tokens per second)\n",
      "llama_print_timings:        eval time =    8496.39 ms /    38 runs   (  223.59 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   10870.90 ms /   113 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 93 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Sensitivity: Yes./No.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Sensitivity: Yes./No.'\n",
      "2. Corrections: \n",
      "   - Remove the extra space after 'Yes' and before '/No.'\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Sensitivity: Yes./No.\n",
      "\n",
      "Corrections:\n",
      "- Remove the extra space after 'Yes' and before '/No.'\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Sensitivity: Yes./No.\n",
      "CORRECTIONS: \n",
      " - Remove the extra space after 'Yes' and before '/No.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      28.23 ms /    42 runs   (    0.67 ms per token,  1487.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2707.82 ms /    93 tokens (   29.12 ms per token,    34.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9192.29 ms /    41 runs   (  224.20 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   11963.47 ms /   134 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 83 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Paragraph numbering begins here and it continuous for the rest of the document. Do not include recommendations in paragraph numbering. '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Paragraph numbering begins here and it continues for the rest of the document. Do not include recommendations in paragraph numbering.\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Paragraph numbering begins here and it continues for the rest of the document. Do not include recommendations in paragraph numbering.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Paragraph numbering begins here and it continues for the rest of the document. Do not include recommendations in paragraph numbering.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      23.17 ms /    34 runs   (    0.68 ms per token,  1467.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2532.13 ms /    83 tokens (   30.51 ms per token,    32.78 tokens per second)\n",
      "llama_print_timings:        eval time =    7401.27 ms /    33 runs   (  224.28 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    9987.08 ms /   116 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 109 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Anticipated negative reactions from those affected by the recommendations should be listed here.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Anticipated negative reactions from those affected by the recommendations should be listed here.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Anticipated negative reactions from those affected by the recommendations should be listed here.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Anticipated negative reactions from those affected by the recommendations should be listed here.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      42.74 ms /    62 runs   (    0.69 ms per token,  1450.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3130.26 ms /   109 tokens (   28.72 ms per token,    34.82 tokens per second)\n",
      "llama_print_timings:        eval time =   13650.51 ms /    61 runs   (  223.78 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   16878.07 ms /   170 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 76 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Consider sensitivities from a ‘whole-of-Organisation’ and/or whole-of-Organisation perspective, including key Organisation risks: capability, safety, reputation and security.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Consider sensitivities from a ‘whole-of-Organisation’ and/or whole-of-Organisation perspective, including key Organisation risks: capability, safety, reputation and security.'\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Consider sensitivities from a ‘whole-of-Organisation’ and/or whole-of-Organisation perspective, including key Organisation risks: capability, safety, reputation and security.\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Consider sensitivities from a ‘whole-of-Organisation’ and/or whole-of-Organisation perspective, including key Organisation risks: capability, safety, reputation and security.\n",
      "CORRECTIONS: \n",
      " [No corrections needed]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      18.16 ms /    27 runs   (    0.67 ms per token,  1487.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2154.99 ms /    76 tokens (   28.36 ms per token,    35.27 tokens per second)\n",
      "llama_print_timings:        eval time =    5824.61 ms /    26 runs   (  224.02 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    8020.42 ms /   102 tokens\n",
      "Llama.generate: 59 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Financial Impacts: Yes./No.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Financial Impacts: Yes./No.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Financial Impacts: Yes./No.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Financial Impacts: Yes./No.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      17.12 ms /    26 runs   (    0.66 ms per token,  1518.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2173.89 ms /    73 tokens (   29.78 ms per token,    33.58 tokens per second)\n",
      "llama_print_timings:        eval time =    5576.00 ms /    25 runs   (  223.04 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:       total time =    7788.13 ms /    98 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 92 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Financial implications must be detailed here.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Financial implications must be detailed here.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Financial implications must be detailed here.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Financial implications must be detailed here.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      44.43 ms /    64 runs   (    0.69 ms per token,  1440.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2651.99 ms /    92 tokens (   28.83 ms per token,    34.69 tokens per second)\n",
      "llama_print_timings:        eval time =   14107.52 ms /    63 runs   (  223.93 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   16859.86 ms /   155 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 94 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'You must consult the Chief Financial Officer when there are financial implications. List who has been consulted in the consultation section below.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'You must consult the Chief Financial Officer when there are financial implications. List who has been consulted in the consultation section below.'\n",
      "2. Corrections: \n",
      "    - Remove extra space after \"financial\" and before \"implications.\"\n",
      "<|end|>\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "You must consult the Chief Financial Officer when there are financial implications. List who has been consulted in the consultation section below.\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after \"financial\" and before \"implications.\"\n",
      "<|end|>\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " You must consult the Chief Financial Officer when there are financial implications. List who has been consulted in the consultation section below.\n",
      "CORRECTIONS: \n",
      " - Remove extra space after \"financial\" and before \"implications.\"\n",
      "<|end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      46.86 ms /    70 runs   (    0.67 ms per token,  1493.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2685.93 ms /    94 tokens (   28.57 ms per token,    35.00 tokens per second)\n",
      "llama_print_timings:        eval time =   15435.89 ms /    69 runs   (  223.71 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   18230.32 ms /   163 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 80 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Financial implication must be considered from a whole-of-Organisation and/or a whole-of-Organisation perspective.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Financial implications must be considered from a whole-of-Organisation and/or a whole-of-Organisation perspective.'\n",
      "2. Corrections: \n",
      "    - Replace \"financial implication\" with \"financial implications\".\n",
      "    - Remove extra space after \"perspective.\"\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Financial implications must be considered from a whole-of-Organisation and/or a whole-of-Organisation perspective.\n",
      "\n",
      "Corrections:\n",
      "- Replace \"financial implication\" with \"financial implications\".\n",
      "    - Remove extra space after \"perspective.\"\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Financial implications must be considered from a whole-of-Organisation and/or a whole-of-Organisation perspective.\n",
      "CORRECTIONS: \n",
      " - Replace \"financial implication\" with \"financial implications\".\n",
      "    - Remove extra space after \"perspective.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      28.52 ms /    43 runs   (    0.66 ms per token,  1507.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2271.33 ms /    80 tokens (   28.39 ms per token,    35.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9409.77 ms /    42 runs   (  224.04 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   11746.86 ms /   122 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 75 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Systems/Policy/deregulation: Yes./No.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Systems/Policy/deregulation: Yes./No.'\n",
      "2. Corrections: \n",
      "   - Remove the extra space after 'Yes' and before '/No.'\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Systems/Policy/deregulation: Yes./No.\n",
      "\n",
      "Corrections:\n",
      "- Remove the extra space after 'Yes' and before '/No.'\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Systems/Policy/deregulation: Yes./No.\n",
      "CORRECTIONS: \n",
      " - Remove the extra space after 'Yes' and before '/No.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      16.69 ms /    25 runs   (    0.67 ms per token,  1498.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2379.04 ms /    75 tokens (   31.72 ms per token,    31.53 tokens per second)\n",
      "llama_print_timings:        eval time =    5388.09 ms /    24 runs   (  224.50 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:       total time =    7804.34 ms /    99 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 80 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'List relevant Policy and how it applies.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: List relevant policies and how they apply.\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "List relevant policies and how they apply.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " List relevant policies and how they apply.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      30.19 ms /    45 runs   (    0.67 ms per token,  1490.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2373.88 ms /    80 tokens (   29.67 ms per token,    33.70 tokens per second)\n",
      "llama_print_timings:        eval time =    9829.49 ms /    44 runs   (  223.40 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:       total time =   12271.01 ms /   124 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 86 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'If applicable, detail the managers accountabilities under policy and regulation.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: If applicable, detail the manager's accountabilities under policy and regulation.'\n",
      "2. Corrections: \n",
      "   - Replace \"managers\" with \"manager's\".\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "If applicable, detail the manager's accountabilities under policy and regulation.\n",
      "\n",
      "Corrections:\n",
      "- Replace \"managers\" with \"manager's\".\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " If applicable, detail the manager's accountabilities under policy and regulation.\n",
      "CORRECTIONS: \n",
      " - Replace \"managers\" with \"manager's\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      35.64 ms /    52 runs   (    0.69 ms per token,  1458.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2523.57 ms /    86 tokens (   29.34 ms per token,    34.08 tokens per second)\n",
      "llama_print_timings:        eval time =   11432.75 ms /    51 runs   (  224.17 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   14036.22 ms /   137 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 81 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'State new or amended regulations, application forms or any other legal material affected by the recommendations.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'State new or amended regulations, application forms or any other legal material affected by the recommendations.'\n",
      "2. Corrections: \n",
      "    - Remove extra space after \"regulations\" and before \"application forms.\"\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "State new or amended regulations, application forms or any other legal material affected by the recommendations.\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after \"regulations\" and before \"application forms.\"\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " State new or amended regulations, application forms or any other legal material affected by the recommendations.\n",
      "CORRECTIONS: \n",
      " - Remove extra space after \"regulations\" and before \"application forms.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      58.77 ms /    85 runs   (    0.69 ms per token,  1446.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2419.59 ms /    81 tokens (   29.87 ms per token,    33.48 tokens per second)\n",
      "llama_print_timings:        eval time =   18791.87 ms /    84 runs   (  223.71 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   21347.06 ms /   165 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 86 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Indicate expected effects to compliance costs for industry and suggest offset mechanisms.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Indicate expected effects on compliance costs for industry and suggest offset mechanisms.'\n",
      "2. Corrections: \n",
      "    - Replace \"expected\" with \"anticipated\" (both words mean the same thing, but \"anticipated\" is more commonly used in business contexts)\n",
      "    - Remove extra space after 'compliance'\n",
      "<|end|>\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Indicate expected effects on compliance costs for industry and suggest offset mechanisms.\n",
      "\n",
      "Corrections:\n",
      "- Replace \"expected\" with \"anticipated\" (both words mean the same thing, but \"anticipated\" is more commonly used in business contexts)\n",
      "    - Remove extra space after 'compliance'\n",
      "<|end|>\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Indicate expected effects on compliance costs for industry and suggest offset mechanisms.\n",
      "CORRECTIONS: \n",
      " - Replace \"expected\" with \"anticipated\" (both words mean the same thing, but \"anticipated\" is more commonly used in business contexts)\n",
      "    - Remove extra space after 'compliance'\n",
      "<|end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      24.19 ms /    36 runs   (    0.67 ms per token,  1488.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2520.52 ms /    86 tokens (   29.31 ms per token,    34.12 tokens per second)\n",
      "llama_print_timings:        eval time =    7844.99 ms /    35 runs   (  224.14 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   10421.26 ms /   121 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 88 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'If applicable, address legislative, procurement and contractual risks associated with the topic.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: If applicable, address legislative, procurement and contractual risks associated with the topic.\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "If applicable, address legislative, procurement and contractual risks associated with the topic.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " If applicable, address legislative, procurement and contractual risks associated with the topic.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      24.89 ms /    37 runs   (    0.67 ms per token,  1486.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2660.59 ms /    88 tokens (   30.23 ms per token,    33.08 tokens per second)\n",
      "llama_print_timings:        eval time =    8057.05 ms /    36 runs   (  223.81 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   10774.20 ms /   124 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 75 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Use the <Organisational Style Guide > for information on how to format regulations and legal cases. '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Use the <Organisational Style Guide > for information on how to format regulations and legal cases.\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Use the <Organisational Style Guide > for information on how to format regulations and legal cases.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Use the <Organisational Style Guide > for information on how to format regulations and legal cases.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      15.83 ms /    24 runs   (    0.66 ms per token,  1516.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2166.95 ms /    75 tokens (   28.89 ms per token,    34.61 tokens per second)\n",
      "llama_print_timings:        eval time =    5140.98 ms /    23 runs   (  223.52 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    7343.74 ms /    98 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 117 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Consultation: Yes./No.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Consultation: Yes./No.\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Consultation: Yes./No.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Consultation: Yes./No.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      48.59 ms /    70 runs   (    0.69 ms per token,  1440.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3656.43 ms /   117 tokens (   31.25 ms per token,    32.00 tokens per second)\n",
      "llama_print_timings:        eval time =   15452.54 ms /    69 runs   (  223.95 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   19221.57 ms /   186 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 126 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Show that there has been broad consultation across the Organisation, and, where relevant, international agencies. Consider what other Groups or Services may have valid input to your advice and, if so, consult with that Group or Service and note that consultation.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Show that there has been broad consultation across the Organisation, and, where relevant, international agencies. Consider what other Groups or Services may have valid input to your advice and, if so, consult with that Group or Service and note that consultation.'\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Show that there has been broad consultation across the Organisation, and, where relevant, international agencies. Consider what other Groups or Services may have valid input to your advice and, if so, consult with that Group or Service and note that consultation.\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Show that there has been broad consultation across the Organisation, and, where relevant, international agencies. Consider what other Groups or Services may have valid input to your advice and, if so, consult with that Group or Service and note that consultation.\n",
      "CORRECTIONS: \n",
      " [No corrections needed]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      56.29 ms /    84 runs   (    0.67 ms per token,  1492.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3668.99 ms /   126 tokens (   29.12 ms per token,    34.34 tokens per second)\n",
      "llama_print_timings:        eval time =   18599.66 ms /    83 runs   (  224.09 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   22404.31 ms /   209 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 97 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Provide the full details of each person who has been consulted in separate numbered paragraphs. <Title written out in full> <First name> <Surname> <Post-nominals>,<Position>, <Group>, or <Department> or <Organisation>.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Provide the full details of each person who has been consulted in separate numbered paragraphs. <Title written out in full> <First name> <Surname> <Post-nominals>,<Position>, <Group>, or <Department> or <Organisation>.'\n",
      "2. Corrections: [ Make a numbered list of your corrections ]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Provide the full details of each person who has been consulted in separate numbered paragraphs. <Title written out in full> <First name> <Surname> <Post-nominals>,<Position>, <Group>, or <Department> or <Organisation>.\n",
      "\n",
      "Corrections:\n",
      "[ Make a numbered list of your corrections ]\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Provide the full details of each person who has been consulted in separate numbered paragraphs. <Title written out in full> <First name> <Surname> <Post-nominals>,<Position>, <Group>, or <Department> or <Organisation>.\n",
      "CORRECTIONS: \n",
      " [ Make a numbered list of your corrections ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      32.06 ms /    47 runs   (    0.68 ms per token,  1466.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2869.20 ms /    97 tokens (   29.58 ms per token,    33.81 tokens per second)\n",
      "llama_print_timings:        eval time =   10298.33 ms /    46 runs   (  223.88 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   13241.14 ms /   143 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 92 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Briefs that refer to the Organisational Strategic Review must be prepared in consultation with the Organisational Strategic Taskforce Review Board'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Briefs that refer to the Organisational Strategic Review must be prepared in consultation with the Organisational Strategic Taskforce Review Board.\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Briefs that refer to the Organisational Strategic Review must be prepared in consultation with the Organisational Strategic Taskforce Review Board.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Briefs that refer to the Organisational Strategic Review must be prepared in consultation with the Organisational Strategic Taskforce Review Board.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      29.47 ms /    43 runs   (    0.69 ms per token,  1459.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2606.96 ms /    92 tokens (   28.34 ms per token,    35.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9389.83 ms /    42 runs   (  223.57 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   12064.07 ms /   134 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 138 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Where whole-of-organisation views are required, access must occur at the (minimum) level with relevant agencies.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Where whole-of-organisation views are required, access must occur at the (minimum) level with relevant agencies.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Where whole-of-organisation views are required, access must occur at the (minimum) level with relevant agencies.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Where whole-of-organisation views are required, access must occur at the (minimum) level with relevant agencies.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      65.87 ms /    96 runs   (    0.69 ms per token,  1457.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3891.38 ms /   138 tokens (   28.20 ms per token,    35.46 tokens per second)\n",
      "llama_print_timings:        eval time =   21318.49 ms /    95 runs   (  224.41 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   25364.51 ms /   233 tokens\n",
      "Llama.generate: 58 prefix-match hit, remaining 99 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Attachment(s): Remove brackets and ‘s’ if applicable, or delete this section if there are no attachments. Name attachments A-Z in the order they are referred to in the brief. Use brackets and underline the words when noting attachments in the body of the brief e.g. (Attachment A).'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Attachment(s): Remove brackets and ‘s’ if applicable, or delete this section if there are no attachments. Name attachments A-Z in the order they are referred to in the brief. Use brackets and underline the words when noting attachments in the body of the brief e.g. (Attachment A).'\n",
      "2. Corrections: [ Make a numbered list of your corrections ]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Attachment(s): Remove brackets and ‘s’ if applicable, or delete this section if there are no attachments. Name attachments A-Z in the order they are referred to in the brief. Use brackets and underline the words when noting attachments in the body of the brief e.g. (Attachment A).\n",
      "\n",
      "Corrections:\n",
      "[ Make a numbered list of your corrections ]\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Attachment(s): Remove brackets and ‘s’ if applicable, or delete this section if there are no attachments. Name attachments A-Z in the order they are referred to in the brief. Use brackets and underline the words when noting attachments in the body of the brief e.g. (Attachment A).\n",
      "CORRECTIONS: \n",
      " [ Make a numbered list of your corrections ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      38.88 ms /    55 runs   (    0.71 ms per token,  1414.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2788.48 ms /    99 tokens (   28.17 ms per token,    35.50 tokens per second)\n",
      "llama_print_timings:        eval time =   12160.25 ms /    54 runs   (  225.19 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:       total time =   15036.76 ms /   153 tokens\n",
      "Llama.generate: 58 prefix-match hit, remaining 100 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Attachment A\tThe file name that appears in PDMS and the headings of each attachment should be self-explanatory and match the document details provided here.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Attachment A - The file name that appears in PDMS and the headings of each attachment should be self-explanatory and match the document details provided here.'\n",
      "2. Corrections: [No formatting errors found.]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Attachment A - The file name that appears in PDMS and the headings of each attachment should be self-explanatory and match the document details provided here.\n",
      "\n",
      "Corrections:\n",
      "[No formatting errors found.]\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Attachment A - The file name that appears in PDMS and the headings of each attachment should be self-explanatory and match the document details provided here.\n",
      "CORRECTIONS: \n",
      " [No formatting errors found.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      39.31 ms /    56 runs   (    0.70 ms per token,  1424.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2812.77 ms /   100 tokens (   28.13 ms per token,    35.55 tokens per second)\n",
      "llama_print_timings:        eval time =   12340.10 ms /    55 runs   (  224.37 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   15242.25 ms /   155 tokens\n",
      "Llama.generate: 58 prefix-match hit, remaining 84 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Attachment B\tIf the brief refers to any previous submissions or correspondence include them as attachments unless there are sensitivities (e.g Limited Distribution).'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Attachment B - If the brief refers to any previous submissions or correspondence, include them as attachments unless there are sensitivities (e.g., Limited Distribution).'\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Attachment B - If the brief refers to any previous submissions or correspondence, include them as attachments unless there are sensitivities (e.g., Limited Distribution).\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Attachment B - If the brief refers to any previous submissions or correspondence, include them as attachments unless there are sensitivities (e.g., Limited Distribution).\n",
      "CORRECTIONS: \n",
      " [No corrections needed]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      23.43 ms /    35 runs   (    0.67 ms per token,  1494.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2351.64 ms /    84 tokens (   28.00 ms per token,    35.72 tokens per second)\n",
      "llama_print_timings:        eval time =    7639.24 ms /    34 runs   (  224.68 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:       total time =   10043.70 ms /   118 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 90 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Attachment C\t\tProvide talking points if required. Ensure they are appropriately cleared.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Attachment C\tProvide talking points if required. Ensure they are appropriately cleared.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Attachment C\tProvide talking points if required. Ensure they are appropriately cleared.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Attachment C\tProvide talking points if required. Ensure they are appropriately cleared.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      27.78 ms /    41 runs   (    0.68 ms per token,  1475.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2574.70 ms /    90 tokens (   28.61 ms per token,    34.96 tokens per second)\n",
      "llama_print_timings:        eval time =    8958.51 ms /    40 runs   (  223.96 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   11595.39 ms /   130 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 88 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Background: Begin the Background after Attachments when a significant space exists. Otherwise, insert a page break above.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Background: Begin the Background after Attachments when a significant space exists. Otherwise, insert a page break above.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Background: Begin the Background after Attachments when a significant space exists. Otherwise, insert a page break above.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Background: Begin the Background after Attachments when a significant space exists. Otherwise, insert a page break above.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      38.68 ms /    56 runs   (    0.69 ms per token,  1447.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2485.02 ms /    88 tokens (   28.24 ms per token,    35.41 tokens per second)\n",
      "llama_print_timings:        eval time =   12300.29 ms /    55 runs   (  223.64 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   14871.97 ms /   143 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 94 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Subheadings can be used to assist the reader (do not use acronyms in these)'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Subheadings can be used to assist the reader (do not use acronyms in these).'\n",
      "2. Corrections: \n",
      "   - Replace \"acronyms\" with \"abbreviations\".\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Subheadings can be used to assist the reader (do not use acronyms in these).\n",
      "\n",
      "Corrections:\n",
      "- Replace \"acronyms\" with \"abbreviations\".\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Subheadings can be used to assist the reader (do not use acronyms in these).\n",
      "CORRECTIONS: \n",
      " - Replace \"acronyms\" with \"abbreviations\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      56.80 ms /    82 runs   (    0.69 ms per token,  1443.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2789.30 ms /    94 tokens (   29.67 ms per token,    33.70 tokens per second)\n",
      "llama_print_timings:        eval time =   18159.19 ms /    81 runs   (  224.19 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   21078.02 ms /   175 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 78 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Write no more than one page of background (or synthesise the background materials) and provide additional detail in an attachment if required.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Write no more than one page of background (or synthesize the background materials) and provide additional detail in an attachment if required.'\n",
      "2. Corrections: \n",
      "   - Remove extra space after 'Write'\n",
      "   - Change 'no more than one page' to 'up to one page'\n",
      "   - Add a period at the end of the sentence\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Write no more than one page of background (or synthesize the background materials) and provide additional detail in an attachment if required.\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after 'Write'\n",
      "   - Change 'no more than one page' to 'up to one page'\n",
      "   - Add a period at the end of the sentence\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Write no more than one page of background (or synthesize the background materials) and provide additional detail in an attachment if required.\n",
      "CORRECTIONS: \n",
      " - Remove extra space after 'Write'\n",
      "   - Change 'no more than one page' to 'up to one page'\n",
      "   - Add a period at the end of the sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      26.89 ms /    40 runs   (    0.67 ms per token,  1487.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2418.12 ms /    78 tokens (   31.00 ms per token,    32.26 tokens per second)\n",
      "llama_print_timings:        eval time =    8731.56 ms /    39 runs   (  223.89 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   11211.22 ms /   117 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 80 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Explain why the manager needs to know the information.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Explain why the manager needs to know the information.'\n",
      "2. Corrections: \n",
      "   - Remove extra space after period at end of sentence.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Explain why the manager needs to know the information.\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after period at end of sentence.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Explain why the manager needs to know the information.\n",
      "CORRECTIONS: \n",
      " - Remove extra space after period at end of sentence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      19.58 ms /    30 runs   (    0.65 ms per token,  1532.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2339.74 ms /    80 tokens (   29.25 ms per token,    34.19 tokens per second)\n",
      "llama_print_timings:        eval time =    6492.86 ms /    29 runs   (  223.89 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    8876.53 ms /   109 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 77 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Do not repeat information in the background provided elsewhere in the brief.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Do not repeat information in the background provided elsewhere in the brief.\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Do not repeat information in the background provided elsewhere in the brief.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Do not repeat information in the background provided elsewhere in the brief.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      16.61 ms /    25 runs   (    0.66 ms per token,  1505.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2372.31 ms /    77 tokens (   30.81 ms per token,    32.46 tokens per second)\n",
      "llama_print_timings:        eval time =    5394.66 ms /    24 runs   (  224.78 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:       total time =    7805.02 ms /   101 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 84 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Remember, the managers are time poor. '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Remember, the managers are time poor.\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Remember, the managers are time poor.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Remember, the managers are time poor.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      22.86 ms /    34 runs   (    0.67 ms per token,  1487.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2468.84 ms /    84 tokens (   29.39 ms per token,    34.02 tokens per second)\n",
      "llama_print_timings:        eval time =    7384.63 ms /    33 runs   (  223.78 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    9904.41 ms /   117 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 94 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Organisation is to consider how to provide complex issues succinctly and accessibly.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Organisation is to consider how to provide complex issues succinctly and accessibly.\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Organisation is to consider how to provide complex issues succinctly and accessibly.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Organisation is to consider how to provide complex issues succinctly and accessibly.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      29.99 ms /    45 runs   (    0.67 ms per token,  1500.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2666.29 ms /    94 tokens (   28.36 ms per token,    35.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9858.36 ms /    44 runs   (  224.05 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   12593.70 ms /   138 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 96 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Use numbered paragraphs. Write one to three sentences per paragraph. Sentences are clearer when they contain only one idea.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Use numbered paragraphs. Write one to three sentences per paragraph. Sentences are clearer when they contain only one idea.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Use numbered paragraphs. Write one to three sentences per paragraph. Sentences are clearer when they contain only one idea.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Use numbered paragraphs. Write one to three sentences per paragraph. Sentences are clearer when they contain only one idea.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      29.84 ms /    44 runs   (    0.68 ms per token,  1474.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2825.70 ms /    96 tokens (   29.43 ms per token,    33.97 tokens per second)\n",
      "llama_print_timings:        eval time =    9634.96 ms /    43 runs   (  224.07 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   12528.32 ms /   139 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 79 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Ensure the security classification is on each page, top and bottom (14 point, bold, RED, Calibri Light).'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Ensure the security classification is on each page, top and bottom (14 point, bold, RED, Calibri Light).\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Ensure the security classification is on each page, top and bottom (14 point, bold, RED, Calibri Light).\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Ensure the security classification is on each page, top and bottom (14 point, bold, RED, Calibri Light).\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      20.27 ms /    30 runs   (    0.68 ms per token,  1480.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2337.24 ms /    79 tokens (   29.59 ms per token,    33.80 tokens per second)\n",
      "llama_print_timings:        eval time =    6511.25 ms /    29 runs   (  224.53 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:       total time =    8893.61 ms /   108 tokens\n",
      "Llama.generate: 67 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Brief dot point format – Paragraph level 1.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Brief dot point format – Paragraph level 1.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Brief dot point format – Paragraph level 1.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Brief dot point format – Paragraph level 1.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      20.43 ms /    30 runs   (    0.68 ms per token,  1468.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2111.50 ms /    69 tokens (   30.60 ms per token,    32.68 tokens per second)\n",
      "llama_print_timings:        eval time =    6530.94 ms /    29 runs   (  225.20 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:       total time =    8687.93 ms /    98 tokens\n",
      "Llama.generate: 67 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Brief dot point format – Paragraph level 2.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Brief dot point format – Paragraph level 2.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Brief dot point format – Paragraph level 2.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Brief dot point format – Paragraph level 2.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      19.92 ms /    30 runs   (    0.66 ms per token,  1505.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2173.53 ms /    69 tokens (   31.50 ms per token,    31.75 tokens per second)\n",
      "llama_print_timings:        eval time =    6488.80 ms /    29 runs   (  223.75 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    8707.21 ms /    98 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Brief dot point format – Paragraph level 3.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Brief dot point format – Paragraph level 3.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Brief dot point format – Paragraph level 3.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Brief dot point format – Paragraph level 3.\n",
      "CORRECTIONS: \n",
      " None needed.\n",
      "IN Table\n",
      "INFO:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      17.62 ms /    27 runs   (    0.65 ms per token,  1532.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2083.68 ms /    69 tokens (   30.20 ms per token,    33.11 tokens per second)\n",
      "llama_print_timings:        eval time =    5812.71 ms /    26 runs   (  223.57 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    7936.70 ms /    95 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 71 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'INFO:'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: INFO:\n",
      "2. Corrections: \n",
      "   - Remove extra space after 'INFO:'\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "INFO:\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after 'INFO:'\n",
      "DONE process_llm_output\n",
      "Choose Manager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      14.51 ms /    22 runs   (    0.66 ms per token,  1516.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2134.52 ms /    71 tokens (   30.06 ms per token,    33.26 tokens per second)\n",
      "llama_print_timings:        eval time =    4707.69 ms /    21 runs   (  224.18 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    6874.05 ms /    92 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 94 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Choose Manager'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Choose Manager'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Choose Manager\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "Reason for Action Date: Required. Why does the Manager need to take action by this date?. Do not use ‘routine’.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      40.58 ms /    59 runs   (    0.69 ms per token,  1453.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2709.88 ms /    94 tokens (   28.83 ms per token,    34.69 tokens per second)\n",
      "llama_print_timings:        eval time =   12996.76 ms /    58 runs   (  224.08 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   15798.53 ms /   152 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Reason for Action Date: Required. Why does the Manager need to take action by this date?. Do not use ‘routine’.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Reason for Action Date: Required. Why does the Manager need to take action by this date? Do not use ‘routine’. '\n",
      "2. Corrections: \n",
      "   a) Remove extra space after \"action\" and before \"by\".\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Reason for Action Date: Required. Why does the Manager need to take action by this date? Do not use ‘routine’.\n",
      "\n",
      "Corrections:\n",
      "a) Remove extra space after \"action\" and before \"by\".\n",
      "DONE process_llm_output\n",
      "\n",
      "THROUGH:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      19.52 ms /    30 runs   (    0.65 ms per token,  1536.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2362.17 ms /    72 tokens (   32.81 ms per token,    30.48 tokens per second)\n",
      "llama_print_timings:        eval time =    6496.76 ms /    29 runs   (  224.03 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    8904.05 ms /   101 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 79 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'THROUGH:'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Through:\n",
      "2. Corrections: \n",
      "   - Remove extra space after \"THROUGH:\"\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Through:\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after \"THROUGH:\"\n",
      "DONE process_llm_output\n",
      "SEC\tCTO\tCSO\tLEGAL \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      18.44 ms /    28 runs   (    0.66 ms per token,  1518.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2315.36 ms /    79 tokens (   29.31 ms per token,    34.12 tokens per second)\n",
      "llama_print_timings:        eval time =    6063.33 ms /    27 runs   (  224.57 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:       total time =    8420.48 ms /   106 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'SEC\tCTO\tCSO\tLEGAL '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: SEC CTO CSO LEGAL\n",
      "2. Corrections: No formatting errors found.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "SEC CTO CSO LEGAL\n",
      "\n",
      "Corrections:\n",
      "No formatting errors found.\n",
      "DONE process_llm_output\n",
      "\n",
      "CC:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      17.78 ms /    27 runs   (    0.66 ms per token,  1518.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1992.62 ms /    69 tokens (   28.88 ms per token,    34.63 tokens per second)\n",
      "llama_print_timings:        eval time =    5814.22 ms /    26 runs   (  223.62 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    7847.40 ms /    95 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 87 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'CC:'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: CC:'\n",
      "2. Corrections: \n",
      "   - Remove extra space after 'CC:'\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "CC:\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after 'CC:'\n",
      "DONE process_llm_output\n",
      "SEC, CFO FinOps, CYBER, ITSEC , FSST LG \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      26.91 ms /    39 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2537.08 ms /    87 tokens (   29.16 ms per token,    34.29 tokens per second)\n",
      "llama_print_timings:        eval time =    8555.24 ms /    38 runs   (  225.14 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:       total time =   11152.34 ms /   125 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'SEC, CFO FinOps, CYBER, ITSEC , FSST LG '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: SEC, CFO FinOps, CYBER, ITSEC , FSST LG\n",
      "2. Corrections: No formatting errors found.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "SEC, CFO FinOps, CYBER, ITSEC , FSST LG\n",
      "\n",
      "Corrections:\n",
      "No formatting errors found.\n",
      "DONE process_llm_output\n",
      "\n",
      "PDR SUBJECT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      14.23 ms /    21 runs   (    0.68 ms per token,  1475.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2182.55 ms /    72 tokens (   30.31 ms per token,    32.99 tokens per second)\n",
      "llama_print_timings:        eval time =    4471.17 ms /    20 runs   (  223.56 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    6685.75 ms /    92 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 71 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'PDR SUBJECT'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: PDR SUBJECT\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "PDR SUBJECT\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n",
      "Key Issues:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      95.85 ms /   141 runs   (    0.68 ms per token,  1470.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2200.10 ms /    71 tokens (   30.99 ms per token,    32.27 tokens per second)\n",
      "llama_print_timings:        eval time =   31337.57 ms /   140 runs   (  223.84 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   33771.44 ms /   211 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 108 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Key Issues:'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Key Issues:\n",
      "2. Remove any formatting errors such as extra spaces\n",
      "**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      "1. Key Issues:\n",
      "2. Remove any formatting errors such as extra spaces\n",
      "**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "[ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "\n",
      "Corrections:\n",
      "[ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      "1. Key Issues:\n",
      "2. Remove any formatting errors such as extra spaces\n",
      "**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "DONE process_llm_output\n",
      "Template guidance and sections that need the drafting officer to complete are in blue. When the brief has been finalised, remove this text. All text other than the security classification should be changed to black.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      45.72 ms /    66 runs   (    0.69 ms per token,  1443.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3149.28 ms /   108 tokens (   29.16 ms per token,    34.29 tokens per second)\n",
      "llama_print_timings:        eval time =   14551.52 ms /    65 runs   (  223.87 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   17805.05 ms /   173 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 105 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Template guidance and sections that need the drafting officer to complete are in blue. When the brief has been finalised, remove this text. All text other than the security classification should be changed to black.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Template guidance and sections that need the drafting officer to complete are in blue. When the brief has been finalised, remove this text. All text other than the security classification should be changed to black.'\n",
      "2. Corrections: [ Make a numbered list of your corrections ]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Template guidance and sections that need the drafting officer to complete are in blue. When the brief has been finalised, remove this text. All text other than the security classification should be changed to black.\n",
      "\n",
      "Corrections:\n",
      "[ Make a numbered list of your corrections ]\n",
      "DONE process_llm_output\n",
      "The front of the brief is formatted in a table. Select ‘show paragraph marks’ (Ctrl+*) and turn on ‘view gridlines’ to assist in formatting this document.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      40.00 ms /    57 runs   (    0.70 ms per token,  1424.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3012.25 ms /   105 tokens (   28.69 ms per token,    34.86 tokens per second)\n",
      "llama_print_timings:        eval time =   12557.78 ms /    56 runs   (  224.25 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   15659.64 ms /   161 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'The front of the brief is formatted in a table. Select ‘show paragraph marks’ (Ctrl+*) and turn on ‘view gridlines’ to assist in formatting this document.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: The front of the brief is formatted in a table. Select ‘show paragraph marks’ (Ctrl+*) and turn on ‘view gridlines’ to assist in formatting this document.'\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "The front of the brief is formatted in a table. Select ‘show paragraph marks’ (Ctrl+*) and turn on ‘view gridlines’ to assist in formatting this document.\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      30.94 ms /    46 runs   (    0.67 ms per token,  1486.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2029.47 ms /    68 tokens (   29.85 ms per token,    33.51 tokens per second)\n",
      "llama_print_timings:        eval time =   10085.82 ms /    45 runs   (  224.13 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   12183.79 ms /   113 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 145 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  ''\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: \"The quick brown fox jumps over the lazy dog.\"\n",
      "2. Corrections: \n",
      "   .1. Replace \"lazy\" with \"lethargic\".\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "Corrections:\n",
      ".1. Replace \"lazy\" with \"lethargic\".\n",
      "DONE process_llm_output\n",
      "Keep the key issues section to eight lines of text (a two-minute read). Use plain language and choose everyday words. The key issues should clearly explain what the action requested is and why it is required. The brief should provide a whole-of-organisation view, and where applicable, a whole-of-organisation view – not numerous single Group or Function views. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      93.11 ms /   135 runs   (    0.69 ms per token,  1449.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4069.59 ms /   145 tokens (   28.07 ms per token,    35.63 tokens per second)\n",
      "llama_print_timings:        eval time =   30065.48 ms /   134 runs   (  224.37 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   34361.36 ms /   279 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 86 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Keep the key issues section to eight lines of text (a two-minute read). Use plain language and choose everyday words. The key issues should clearly explain what the action requested is and why it is required. The brief should provide a whole-of-organisation view, and where applicable, a whole-of-organisation view – not numerous single Group or Function views. '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Keep the key issues section to eight lines of text (a two-minute read). Use plain language and choose everyday words. The key issues should clearly explain what the action requested is and why it is required. The brief should provide a whole-of-organisation view, and where applicable, a whole-of-organisation view – not numerous single Group or Function views.\n",
      "2. Corrections: 1) Remove extra spaces between words; 2) Use plain language throughout the text; 3) Ensure that the key issues are clearly explained; 4) Maintain consistency in formatting and punctuation.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Keep the key issues section to eight lines of text (a two-minute read). Use plain language and choose everyday words. The key issues should clearly explain what the action requested is and why it is required. The brief should provide a whole-of-organisation view, and where applicable, a whole-of-organisation view – not numerous single Group or Function views.\n",
      "\n",
      "Corrections:\n",
      "1) Remove extra spaces between words; 2) Use plain language throughout the text; 3) Ensure that the key issues are clearly explained; 4) Maintain consistency in formatting and punctuation.\n",
      "DONE process_llm_output\n",
      "\n",
      "Recommendation(s): (Remove brackets and ‘s’ if applicable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      44.85 ms /    66 runs   (    0.68 ms per token,  1471.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2440.53 ms /    86 tokens (   28.38 ms per token,    35.24 tokens per second)\n",
      "llama_print_timings:        eval time =   14597.09 ms /    65 runs   (  224.57 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:       total time =   17143.10 ms /   151 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Recommendation(s): (Remove brackets and ‘s’ if applicable)'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Recommendation(s): Remove brackets and ‘s’ if applicable.'\n",
      "2. Corrections: \n",
      "   - Remove the extra space after 'Recommendation(s)'\n",
      "   - Change 'Remove' to 'Removing' for better grammar flow\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Recommendation(s): Remove brackets and ‘s’ if applicable.\n",
      "\n",
      "Corrections:\n",
      "- Remove the extra space after 'Recommendation(s)'\n",
      "   - Change 'Remove' to 'Removing' for better grammar flow\n",
      "DONE process_llm_output\n",
      "Decision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      12.91 ms /    19 runs   (    0.68 ms per token,  1472.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2008.72 ms /    70 tokens (   28.70 ms per token,    34.85 tokens per second)\n",
      "llama_print_timings:        eval time =    4046.31 ms /    18 runs   (  224.79 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:       total time =    6084.06 ms /    88 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 93 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Decision'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Decision\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Decision\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n",
      "the action should be in bold with the rest of the recommendation in plain text. Be concise and clear and number each recommendation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      58.25 ms /    86 runs   (    0.68 ms per token,  1476.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2845.49 ms /    93 tokens (   30.60 ms per token,    32.68 tokens per second)\n",
      "llama_print_timings:        eval time =   19112.30 ms /    85 runs   (  224.85 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:       total time =   22097.13 ms /   178 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'the action should be in bold with the rest of the recommendation in plain text. Be concise and clear and number each recommendation.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. The action should be in bold with the rest of the recommendation in plain text. Be concise and clear and number each recommendation.\n",
      "2. Remove any formatting errors such as extra spaces. **IMPORTANT** Use the following template to format your response: **.1. Edited Text:** [Your corrected text here] **2. Corrections:** [Make a numbered list of your corrections]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "** [Your corrected text here] **2. Corrections:** [Make a numbered list of your corrections]\n",
      "\n",
      "Corrections:\n",
      "** [Make a numbered list of your corrections]\n",
      "DONE process_llm_output\n",
      "Approve/Not approved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      14.57 ms /    22 runs   (    0.66 ms per token,  1510.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2139.06 ms /    73 tokens (   29.30 ms per token,    34.13 tokens per second)\n",
      "llama_print_timings:        eval time =    4702.63 ms /    21 runs   (  223.93 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    6874.35 ms /    94 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 86 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Approve/Not approved'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Approve/Not approved\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Approve/Not approved\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n",
      "if the manager must sign specific attachments, make each a separate recommendation for each attachment. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      24.28 ms /    36 runs   (    0.67 ms per token,  1482.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2498.62 ms /    86 tokens (   29.05 ms per token,    34.42 tokens per second)\n",
      "llama_print_timings:        eval time =    7832.91 ms /    35 runs   (  223.80 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   10385.84 ms /   121 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'if the manager must sign specific attachments, make each a separate recommendation for each attachment. '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'If the manager must sign specific attachments, make each a separate recommendation for each attachment.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "If the manager must sign specific attachments, make each a separate recommendation for each attachment.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "Signed/Not signed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      14.70 ms /    22 runs   (    0.67 ms per token,  1496.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2130.95 ms /    73 tokens (   29.19 ms per token,    34.26 tokens per second)\n",
      "llama_print_timings:        eval time =    4712.11 ms /    21 runs   (  224.39 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    6875.78 ms /    94 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 82 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Signed/Not signed'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Signed/Not signed\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Signed/Not signed\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n",
      "add or delete rows to this table to adjust to the number of recommendations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      21.83 ms /    33 runs   (    0.66 ms per token,  1511.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2468.69 ms /    82 tokens (   30.11 ms per token,    33.22 tokens per second)\n",
      "llama_print_timings:        eval time =    7171.92 ms /    32 runs   (  224.12 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    9690.52 ms /   114 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'add or delete rows to this table to adjust to the number of recommendations.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Add or delete rows to this table to adjust to the number of recommendations.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Add or delete rows to this table to adjust to the number of recommendations.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "Agree/Please discuss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      14.42 ms /    22 runs   (    0.66 ms per token,  1525.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2257.71 ms /    73 tokens (   30.93 ms per token,    32.33 tokens per second)\n",
      "llama_print_timings:        eval time =    4705.12 ms /    21 runs   (  224.05 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    6995.60 ms /    94 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 71 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Agree/Please discuss'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Agree/Please discuss\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Agree/Please discuss\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n",
      "Media Considerations:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      23.87 ms /    36 runs   (    0.66 ms per token,  1508.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2223.28 ms /    71 tokens (   31.31 ms per token,    31.93 tokens per second)\n",
      "llama_print_timings:        eval time =    7845.20 ms /    35 runs   (  224.15 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   10123.15 ms /   106 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 85 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Media Considerations:'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Media Considerations:\n",
      "2. Corrections: \n",
      "   - Remove extra space after 'Media' and before 'Considerations:'\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Media Considerations:\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after 'Media' and before 'Considerations:'\n",
      "DONE process_llm_output\n",
      "List any media considerations here. Include any past media interest in this topic. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      22.94 ms /    34 runs   (    0.67 ms per token,  1482.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2554.70 ms /    85 tokens (   30.06 ms per token,    33.27 tokens per second)\n",
      "llama_print_timings:        eval time =    7373.26 ms /    33 runs   (  223.43 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:       total time =    9979.20 ms /   118 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 89 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'List any media considerations here. Include any past media interest in this topic. '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: List any media considerations here. Include any past media interest in this topic.\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "List any media considerations here. Include any past media interest in this topic.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "If no media interest is expected, write: ‘This issue is not expected to generate any media interest.’ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      27.19 ms /    40 runs   (    0.68 ms per token,  1471.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2636.85 ms /    89 tokens (   29.63 ms per token,    33.75 tokens per second)\n",
      "llama_print_timings:        eval time =    8734.01 ms /    39 runs   (  223.95 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   11433.68 ms /   128 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 84 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'If no media interest is expected, write: ‘This issue is not expected to generate any media interest.’ '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'If no media interest is expected, write: \"This issue is not expected to generate any media interest.\"'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "If no media interest is expected, write: \"This issue is not expected to generate any media interest.\"\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "Clear media products (e.g. talking points) before submitting the brief.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      35.98 ms /    52 runs   (    0.69 ms per token,  1445.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2543.88 ms /    84 tokens (   30.28 ms per token,    33.02 tokens per second)\n",
      "llama_print_timings:        eval time =   11423.81 ms /    51 runs   (  224.00 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   14047.65 ms /   135 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 82 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Clear media products (e.g. talking points) before submitting the brief.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Clear media products (e.g., talking points) before submitting the brief.'\n",
      "2. Corrections: \n",
      "   - Replace \"talking\" with its plural form, \"talking points.\"\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Clear media products (e.g., talking points) before submitting the brief.\n",
      "\n",
      "Corrections:\n",
      "- Replace \"talking\" with its plural form, \"talking points.\"\n",
      "DONE process_llm_output\n",
      "\n",
      "Signature: The signature block should be on the same page as the recommendations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      31.59 ms /    46 runs   (    0.69 ms per token,  1456.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2422.07 ms /    82 tokens (   29.54 ms per token,    33.86 tokens per second)\n",
      "llama_print_timings:        eval time =   10047.26 ms /    45 runs   (  223.27 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:       total time =   12539.88 ms /   127 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Signature: The signature block should be on the same page as the recommendations.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Signature: The signature block should be on the same page as the recommendations.'\n",
      "2. Corrections: \n",
      "   - Remove extra space after \"The\" and before \"signature\".\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Signature: The signature block should be on the same page as the recommendations.\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after \"The\" and before \"signature\".\n",
      "DONE process_llm_output\n",
      "\n",
      "The Responsible Manager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      14.87 ms /    22 runs   (    0.68 ms per token,  1479.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2262.23 ms /    73 tokens (   30.99 ms per token,    32.27 tokens per second)\n",
      "llama_print_timings:        eval time =    4684.48 ms /    21 runs   (  223.07 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:       total time =    6980.06 ms /    94 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 75 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'The Responsible Manager'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: The Responsible Manager\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "The Responsible Manager\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n",
      "\tMonth 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      16.68 ms /    25 runs   (    0.67 ms per token,  1498.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2250.05 ms /    75 tokens (   30.00 ms per token,    33.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5368.90 ms /    24 runs   (  223.70 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    7655.77 ms /    99 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 71 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '\tMonth 2024'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Month 2024'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Month 2024\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n",
      "Managers comments:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      23.08 ms /    35 runs   (    0.66 ms per token,  1516.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2148.01 ms /    71 tokens (   30.25 ms per token,    33.05 tokens per second)\n",
      "llama_print_timings:        eval time =    7604.68 ms /    34 runs   (  223.67 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    9803.92 ms /   105 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Managers comments:'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Managers comments:\n",
      "2. Corrections: \n",
      "   - Remove extra space after \"Managers\" and before \"comments:\"\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Managers comments:\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after \"Managers\" and before \"comments:\"\n",
      "DONE process_llm_output\n",
      "\n",
      "Cleared by:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      25.86 ms /    38 runs   (    0.68 ms per token,  1469.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2188.84 ms /    72 tokens (   30.40 ms per token,    32.89 tokens per second)\n",
      "llama_print_timings:        eval time =    8290.24 ms /    37 runs   (  224.06 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   10537.69 ms /   109 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 85 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Cleared by:'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: \"Cleared by:\"\n",
      "2. Corrections: \n",
      "   - Remove extra space after colon (:)\n",
      "<|end|>\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Cleared by:\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after colon (:)\n",
      "<|end|>\n",
      "DONE process_llm_output\n",
      "<Title> <First Name> <Surname> <Post-nominals>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      32.47 ms /    48 runs   (    0.68 ms per token,  1478.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2573.98 ms /    85 tokens (   30.28 ms per token,    33.02 tokens per second)\n",
      "llama_print_timings:        eval time =   10512.83 ms /    47 runs   (  223.68 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   13160.50 ms /   132 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '<Title> <First Name> <Surname> <Post-nominals>'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: <Title>John Smith</First Name><Surname>Doe</Post-nominals>\n",
      "2. Corrections: \n",
      "   - Remove extra spaces between title and first name.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "<Title>John Smith</First Name><Surname>Doe</Post-nominals>\n",
      "\n",
      "Corrections:\n",
      "- Remove extra spaces between title and first name.\n",
      "DONE process_llm_output\n",
      "\n",
      "<Position>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      13.36 ms /    20 runs   (    0.67 ms per token,  1497.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2109.99 ms /    69 tokens (   30.58 ms per token,    32.70 tokens per second)\n",
      "llama_print_timings:        eval time =    4240.50 ms /    19 runs   (  223.18 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:       total time =    6380.10 ms /    88 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '<Position>'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: \"Position\"\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Position\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n",
      "<Group>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      21.05 ms /    32 runs   (    0.66 ms per token,  1520.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2191.52 ms /    69 tokens (   31.76 ms per token,    31.49 tokens per second)\n",
      "llama_print_timings:        eval time =    6944.09 ms /    31 runs   (  224.00 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    9183.32 ms /   100 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '<Group>'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: '<Group>'\n",
      "2. Corrections: \n",
      "   - Remove extra space before and after the word 'group'.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "<Group>\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space before and after the word 'group'.\n",
      "DONE process_llm_output\n",
      "\n",
      "<Phone>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      13.12 ms /    20 runs   (    0.66 ms per token,  1524.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2068.69 ms /    69 tokens (   29.98 ms per token,    33.35 tokens per second)\n",
      "llama_print_timings:        eval time =    4254.76 ms /    19 runs   (  223.93 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    6353.12 ms /    88 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 76 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '<Phone>'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: <Phone>\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "<Phone>\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n",
      "Day Month 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      15.74 ms /    24 runs   (    0.66 ms per token,  1525.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2236.81 ms /    76 tokens (   29.43 ms per token,    33.98 tokens per second)\n",
      "llama_print_timings:        eval time =    5137.41 ms /    23 runs   (  223.37 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:       total time =    7410.31 ms /    99 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Day Month 2024'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Day Month 2024\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Day Month 2024\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n",
      "Primary Contact Officer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      14.09 ms /    21 runs   (    0.67 ms per token,  1490.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2108.07 ms /    72 tokens (   29.28 ms per token,    34.15 tokens per second)\n",
      "llama_print_timings:        eval time =    4470.20 ms /    20 runs   (  223.51 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    6610.15 ms /    92 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Primary Contact Officer: '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Primary Contact Officer:\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Primary Contact Officer:\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "Secondary Contact Officer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      14.58 ms /    22 runs   (    0.66 ms per token,  1509.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2216.72 ms /    73 tokens (   30.37 ms per token,    32.93 tokens per second)\n",
      "llama_print_timings:        eval time =    4692.28 ms /    21 runs   (  223.44 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:       total time =    6942.52 ms /    94 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 85 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Secondary Contact Officer: '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Secondary Contact Officer:\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Secondary Contact Officer:\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n",
      "<Title> <First Name> <Surname> <Post-nominals>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      32.20 ms /    48 runs   (    0.67 ms per token,  1490.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2424.80 ms /    85 tokens (   28.53 ms per token,    35.05 tokens per second)\n",
      "llama_print_timings:        eval time =   10535.93 ms /    47 runs   (  224.17 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   13034.25 ms /   132 tokens\n",
      "Llama.generate: 140 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '<Title> <First Name> <Surname> <Post-nominals>'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: <Title>John Smith</First Name><Surname>Doe</Post-nominals>\n",
      "2. Corrections: \n",
      "   - Remove extra spaces between title and first name.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "<Title>John Smith</First Name><Surname>Doe</Post-nominals>\n",
      "\n",
      "Corrections:\n",
      "- Remove extra spaces between title and first name.\n",
      "DONE process_llm_output\n",
      "<Title> <First Name> <Surname> <Post-nominals>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      32.08 ms /    48 runs   (    0.67 ms per token,  1496.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   10711.88 ms /    48 runs   (  223.16 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:       total time =   10785.21 ms /    48 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '<Title> <First Name> <Surname> <Post-nominals>'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: <Title>John Smith</First Name><Surname>Doe</Post-nominals>\n",
      "2. Corrections: \n",
      "   - Remove extra spaces between title and first name.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "<Title>John Smith</First Name><Surname>Doe</Post-nominals>\n",
      "\n",
      "Corrections:\n",
      "- Remove extra spaces between title and first name.\n",
      "DONE process_llm_output\n",
      "\n",
      "<Position>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      13.45 ms /    20 runs   (    0.67 ms per token,  1487.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2102.25 ms /    69 tokens (   30.47 ms per token,    32.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4256.94 ms /    19 runs   (  224.05 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    6388.88 ms /    88 tokens\n",
      "Llama.generate: 125 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '<Position>'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: \"Position\"\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Position\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "<Position>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      13.76 ms /    20 runs   (    0.69 ms per token,  1454.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    4462.86 ms /    20 runs   (  223.14 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:       total time =    4492.98 ms /    20 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '<Position>'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: \"Position\"\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Position\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n",
      "<Group>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      21.06 ms /    32 runs   (    0.66 ms per token,  1519.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2018.28 ms /    69 tokens (   29.25 ms per token,    34.19 tokens per second)\n",
      "llama_print_timings:        eval time =    6947.12 ms /    31 runs   (  224.10 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    9013.22 ms /   100 tokens\n",
      "Llama.generate: 125 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '<Group>'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: '<Group>'\n",
      "2. Corrections: \n",
      "   - Remove extra space before and after the word 'group'.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "<Group>\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space before and after the word 'group'.\n",
      "DONE process_llm_output\n",
      "<Group>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      21.45 ms /    32 runs   (    0.67 ms per token,  1491.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    7152.93 ms /    32 runs   (  223.53 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    7199.79 ms /    32 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '<Group>'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: '<Group>'\n",
      "2. Corrections: \n",
      "   - Remove extra space before and after the word 'group'.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "<Group>\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space before and after the word 'group'.\n",
      "DONE process_llm_output\n",
      "\n",
      "<Phone>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      13.16 ms /    20 runs   (    0.66 ms per token,  1520.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2014.17 ms /    69 tokens (   29.19 ms per token,    34.26 tokens per second)\n",
      "llama_print_timings:        eval time =    4263.13 ms /    19 runs   (  224.38 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    6306.16 ms /    88 tokens\n",
      "Llama.generate: 125 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '<Phone>'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: <Phone>\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "<Phone>\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "<Phone>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3658.45 ms\n",
      "llama_print_timings:      sample time =      13.10 ms /    20 runs   (    0.66 ms per token,  1526.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    4478.23 ms /    20 runs   (  223.91 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =    4507.53 ms /    20 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '<Phone>'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: <Phone>\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "<Phone>\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#input_file = \"/home/cdsw/data/simple-word-file-with-table.docx\"\n",
    "input_file = \"/home/cdsw/data/full_client_input_doc.docx\"\n",
    "output_file = \"/home/cdsw/data/output_doc.docx\"\n",
    "correction_file = \"/home/cdsw/data/correction_file.docx\"\n",
    "proofread_document(input_file, output_file, correction_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da729b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(input_file, output_file):\n",
    "      # Open the input Word document\n",
    "    doc = Document(input_file)\n",
    "    \n",
    "    for element in doc.paragraphs:\n",
    "        print(\"IN PURE PARAGRAPH\")\n",
    "        print(element.text)\n",
    "        element.add_run('*P')\n",
    "    \n",
    "    # Iterate through all tables in the document\n",
    "    for table in doc.tables:\n",
    "        print(\"IN Table\")        \n",
    "        printed_cells = set()  # To keep track of cells that have been processed\n",
    "        for r_index, row in enumerate(table.rows):\n",
    "            for c_index, cell in enumerate(row.cells):\n",
    "                cell_id = (r_index, c_index)  # Unique identifier for the cell\n",
    "                \n",
    "                # Skip this cell if it is already processed as part of a merged cell\n",
    "                if cell_id in printed_cells:\n",
    "                    continue\n",
    "\n",
    "                # Detect merged cells\n",
    "                is_merged = False\n",
    "                for other_cell in row.cells:\n",
    "                    if other_cell is not cell and other_cell._element is cell._element:\n",
    "                        is_merged = True\n",
    "                        break\n",
    "\n",
    "                # If it's a merged cell, avoid processing duplicates\n",
    "                if is_merged:\n",
    "                    # Register this cell's element to skip duplicates\n",
    "                    for merged_row_index, merged_row in enumerate(table.rows):\n",
    "                        for merged_cell_index, merged_cell in enumerate(merged_row.cells):\n",
    "                            if merged_cell._element is cell._element:\n",
    "                                printed_cells.add((merged_row_index, merged_cell_index))\n",
    "\n",
    "                # Append '**' to the text of the cell if not already processed\n",
    "                if cell.text.strip():  # Check if the cell is not empty\n",
    "#                    cell.text += '*T*B*L'\n",
    "                    for paragraph in cell.paragraphs:\n",
    "                        # Add an asterisk (*)   to the end of each cell paragraph\n",
    "                        print(paragraph.text)\n",
    "                        paragraph.add_run('*T')\n",
    "\n",
    "            print()  # Newline after each row\n",
    "\n",
    "    # Save the modified document to the output file\n",
    "    doc.save(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5bc3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      13.54 ms /    20 runs   (    0.68 ms per token,  1477.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3652.78 ms /   125 tokens (   29.22 ms per token,    34.22 tokens per second)\n",
      "llama_print_timings:        eval time =    4100.43 ms /    19 runs   (  215.81 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:       total time =    7784.64 ms /   144 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'HEADING'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: HEADING\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      15.20 ms /    23 runs   (    0.66 ms per token,  1512.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2041.48 ms /    70 tokens (   29.16 ms per token,    34.29 tokens per second)\n",
      "llama_print_timings:        eval time =    4749.94 ms /    22 runs   (  215.91 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:       total time =    6826.65 ms /    92 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'SECTION 1 '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: 'SECTION 1'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      16.55 ms /    25 runs   (    0.66 ms per token,  1510.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2205.26 ms /    73 tokens (   30.21 ms per token,    33.10 tokens per second)\n",
      "llama_print_timings:        eval time =    5223.78 ms /    24 runs   (  217.66 ms per token,     4.59 tokens per second)\n",
      "llama_print_timings:       total time =    7466.42 ms /    97 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'I am trying something bold here. '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: 'I am trying something bold here.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      14.44 ms /    22 runs   (    0.66 ms per token,  1523.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2520.20 ms /    70 tokens (   36.00 ms per token,    27.78 tokens per second)\n",
      "llama_print_timings:        eval time =    4552.72 ms /    21 runs   (  216.80 ms per token,     4.61 tokens per second)\n",
      "llama_print_timings:       total time =    7105.25 ms /    91 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'SECTION 2 '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: SECTION 2\n",
      "2. Corrections: No corrections needed.\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      14.43 ms /    22 runs   (    0.66 ms per token,  1524.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1988.19 ms /    68 tokens (   29.24 ms per token,    34.20 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.71 ms /    21 runs   (  215.99 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:       total time =    6556.68 ms /    89 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 125 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Product Details'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " **.1. Edited Text:** Product Details\n",
      "**.2. Corrections:** No corrections needed.\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      98.38 ms /   139 runs   (    0.71 ms per token,  1412.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3658.11 ms /   125 tokens (   29.26 ms per token,    34.17 tokens per second)\n",
      "llama_print_timings:        eval time =   30722.95 ms /   138 runs   (  222.63 ms per token,     4.49 tokens per second)\n",
      "llama_print_timings:       total time =   34625.03 ms /   263 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 124 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'I saw a smaple product. These products is below expected quality. \\n I start to understand what he said is quite     right.  The correct value is 12.5  % and 3   r d position. The time is 10:30 am '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: 'I saw a sample product. These products are below expected quality. I start to understand what he said is quite right. The correct value is 12.5% and the third position. The time is 10:30 am.'\n",
      "2. Corrections: [list each correction made]\n",
      "   - Replaced \"smaple\" with \"sample\"\n",
      "   - Changed \"These products is below expected quality.\" to \"These products are below expected quality.\"\n",
      "   - Removed extra space after \"%\" and \"3 r d position.\"\n",
      "   - Added a period at the end of the sentence.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      78.28 ms /   113 runs   (    0.69 ms per token,  1443.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3789.71 ms /   124 tokens (   30.56 ms per token,    32.72 tokens per second)\n",
      "llama_print_timings:        eval time =   24296.99 ms /   112 runs   (  216.94 ms per token,     4.61 tokens per second)\n",
      "llama_print_timings:       total time =   28271.72 ms /   236 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '                   The length of the object is 845mm and cost $1m. The sunrise happens at 0750 EST and visibily is 35 km.      The right way to summarize and favor some advisors is still to be found'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " **.1. Edited Text:** The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibility is 35 km. The right way to summarize and favor some advisors is still to be found.'\n",
      "\n",
      "**2. Corrections:**\n",
      "- Removed extra space after 'The length of the object'\n",
      "- Changed 'cost $1m' to '$1m cost'\n",
      "- Added a period at the end of the sentence\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      12.77 ms /    19 runs   (    0.67 ms per token,  1487.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1959.95 ms /    68 tokens (   28.82 ms per token,    34.69 tokens per second)\n",
      "llama_print_timings:        eval time =    3942.65 ms /    18 runs   (  219.04 ms per token,     4.57 tokens per second)\n",
      "llama_print_timings:       total time =    5930.68 ms /    86 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 129 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Table Section'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: Table Section\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      98.86 ms /   140 runs   (    0.71 ms per token,  1416.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3729.61 ms /   129 tokens (   28.91 ms per token,    34.59 tokens per second)\n",
      "llama_print_timings:        eval time =   30118.07 ms /   139 runs   (  216.68 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:       total time =   34083.63 ms /   268 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Some table to be addd latr here by I. I am trying t  o  make a looooong  setence to see if the LLM is able to capture some of these                       errors such as 2 n    d  or 4 t  h or 6 %s '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: 'Some table to be added later here by I. I am trying t o make a looooong setence to see if the LLM is able to capture some of these errors such as 2 n, d or 4 t h or 6 %s'\n",
      "2. Corrections: [list each correction made]\n",
      "- Replaced 't' with 'to' in \"I am trying t o make a looooong setence\"\n",
      "- Removed extra space after 'by I.'\n",
      "- Changed 'd' to 'and' in \"Some table to be added later here by I. and...\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/home/cdsw/data/simple-word-file-2.docx\"\n",
    "output_file = \"/home/cdsw/data/output_doc.docx\"\n",
    "correction_file = \"/home/cdsw/data/correction_file.docx\"\n",
    "proofread_and_edit_paragraphs(input_file, output_file, correction_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
