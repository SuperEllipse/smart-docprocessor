{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361aa329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -q open-webui python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab8a9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/cdsw/.cache/huggingface/hub/models--TheBloke--Karen_TheEditor_V2_STRICT_Mistral_7B-GGUF/snapshots/6c654aa207a4b673379db7a928b87a01d644676c/karen_theeditor_v2_strict_mistral_7b.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = fpham_karen_theeditor_v2_strict_mistr...\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = fpham_karen_theeditor_v2_strict_mistral_7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7338.66 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 16384\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  1088.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'fpham_karen_theeditor_v2_strict_mistral_7b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "#imports and set up the local model\n",
    "from docx import Document\n",
    "from transformers import pipeline\n",
    "from llama_cpp import Llama\n",
    "from copy import deepcopy\n",
    "from smartdoc_utils import document_postprocessing, process_llm_output\n",
    "model_path = \"/home/cdsw/.cache/huggingface/hub/models--TheBloke--Karen_TheEditor_V2_STRICT_Mistral_7B-GGUF/snapshots/6c654aa207a4b673379db7a928b87a01d644676c/karen_theeditor_v2_strict_mistral_7b.Q8_0.gguf\"\n",
    "# Initialize the model with GPU support\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,  # -1 means use all available GPU layers\n",
    "    n_ctx=16384,  # adjust based on your GPU memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6f0b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_llm_response(text):\n",
    "    #Use the LLM to proofread and edit\n",
    "    # prompt = (\n",
    "    #     \"<|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\\n\"\n",
    "    #     \"<|user|> ** TASK**  \\n\" \n",
    "    #     f\"1. Edit the following text for spelling and grammar mistakes:  '{text}'\\n\"\n",
    "    #     f\"2. Remove any formatting errors such as extra spaces \\n\"\n",
    "    #     f\"**IMPORTANT** use the following template to format your response **.\\n\"\n",
    "    #     f\"1. Edited Text: \\n\"\n",
    "    #     f\"[ Your corrected text here ]\\n\"\n",
    "    #     f\"2. Corrections: \\n\"\n",
    "    #     f\"[ list **each correction made here** ]\\n\"\n",
    "    #     \"<|end|>\\n\"\n",
    "    #     \"<|assistant|>\\n\"\n",
    "    # )\n",
    "    prompt = (\n",
    "        \"<|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\\n\"\n",
    "        \"<|user|> ** TASK**  \\n\" \n",
    "        f\"1. Edit the following text for spelling and grammar mistakes:  '{text}'\\n\"\n",
    "        f\"2. Remove any formatting errors such as extra spaces\"\n",
    "        f\"**IMPORTANT** use the following template to format your response **.\"\n",
    "        f\"1. Edited Text: \"\n",
    "        f\"[ Your corrected text here ]\"\n",
    "        f\"2. Corrections: \"\n",
    "        f\"[ Make a numbered list of your corrections ]\"\n",
    "        \"<|end|>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "    )    \n",
    "#        edited_text = llm(prompt, max_length=len(text) + 50)[0]['generated_text']\n",
    "    edited_text=llm(prompt, max_tokens=400, temperature=0.7, top_p=0.1, top_k=40, repeat_penalty=1.18)\n",
    "    response_text = edited_text['choices'][0]['text'].strip()\n",
    "    print(f\"PROMPT: \\n {prompt} \\n\\n\")\n",
    "    print(f\"LLM RESPONSE: \\n{response_text}\\n\\n\")      \n",
    "    \n",
    "    return response_text\n",
    "    \n",
    "# code that Aamir showed with language_tool checks comes here\n",
    "# Pre-process an input document to handle formatting, \n",
    "# language checks, etc. before further processing\n",
    "#\n",
    "# Args:\n",
    "#   input_doc: The original input document \n",
    "#   corrections_doc: An empty document to populate with corrections \n",
    "#\n",
    "# Returns: \n",
    "#   modified_doc: A modified version of the input document \n",
    "#   corrections_doc: Populated with any corrections made during p\n",
    "def pre_process_document(input_doc, corrections_doc): \n",
    "    \n",
    "    ## REPLACE this with logic of the modified doc\n",
    "    modified_doc = deepcopy(input_doc)\n",
    "    \n",
    "    # logic for pre-processing Fonts, Australian Language Checks, etc. \n",
    "    \n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "def process_document_paragraphs(modified_doc, corrections_doc) : \n",
    "\n",
    "    #modified_doc = deepcopy(input_doc)\n",
    "    # Process each paragraph\n",
    "    for para in modified_doc.paragraphs:\n",
    "        # Store the original formatting\n",
    "        original_runs = para.runs.copy()\n",
    "        \n",
    "        if not para.text.strip():\n",
    "            print(\"Skipping LLM CALL:\")\n",
    "            continue\n",
    "        else:\n",
    "            # Get the text content\n",
    "            text = para.text\n",
    "\n",
    "        llm_output_text = fetch_llm_response(text)\n",
    "\n",
    "        # edits, corrections = document_postprocessing(llm_output_text)\n",
    "        edits, corrections = process_llm_output(llm_output_text)\n",
    "        print(\"EDITS : \\n\", edits)\n",
    "        print(\"CORRECTIONS: \\n\", corrections)\n",
    "        # Clear the paragraph and add the edited text\n",
    "        para.clear()\n",
    "        #para.add_run(edited_text['choices'][0]['text'].strip())\n",
    "        para.add_run(edits)\n",
    "        # Attempt to reapply formatting\n",
    "        new_runs = para.runs\n",
    "        for i, run in enumerate(new_runs):\n",
    "            if i < len(original_runs):\n",
    "                run.font.name = original_runs[i].font.name\n",
    "                run.font.size = original_runs[i].font.size\n",
    "                run.font.bold = original_runs[i].font.bold\n",
    "                run.font.italic = original_runs[i].font.italic\n",
    "                # Add more formatting attributes as needed\n",
    "        \n",
    "\n",
    "        # Let us log the corrections made\n",
    "        corrections_doc.add_paragraph()\n",
    "        corrections_doc.add_paragraph(f\"Original Text : \\n {text}\")\n",
    "        corrections_doc.add_paragraph(f\"Edits : \\n {edits}\")\n",
    "        corrections_doc.add_paragraph(f\"Corrections: \\n{llm_output_text}\")\n",
    "                   \n",
    "    \n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "    \n",
    "def process_document_tables(modified_doc, corrections_doc) : \n",
    "    \n",
    "    ## REPLACE this with logic of the modified doc\n",
    "    #modified_doc = deepcopy(input_doc)\n",
    "    # Iterate through all tables in the document\n",
    "    for table in modified_doc.tables:\n",
    "        print(\"IN Table\")        \n",
    "        printed_cells = set()  # To keep track of cells that have been processed\n",
    "        for r_index, row in enumerate(table.rows):\n",
    "            for c_index, cell in enumerate(row.cells):\n",
    "                cell_id = (r_index, c_index)  # Unique identifier for the cell\n",
    "                \n",
    "                # Skip this cell if it is already processed as part of a merged cell\n",
    "                if cell_id in printed_cells:\n",
    "                    continue\n",
    "\n",
    "                # Detect merged cells\n",
    "                is_merged = False\n",
    "                for other_cell in row.cells:\n",
    "                    if other_cell is not cell and other_cell._element is cell._element:\n",
    "                        is_merged = True\n",
    "                        break\n",
    "\n",
    "                # If it's a merged cell, avoid processing duplicates\n",
    "                if is_merged:\n",
    "                    # Register this cell's element to skip duplicates\n",
    "                    for merged_row_index, merged_row in enumerate(table.rows):\n",
    "                        for merged_cell_index, merged_cell in enumerate(merged_row.cells):\n",
    "                            if merged_cell._element is cell._element:\n",
    "                                printed_cells.add((merged_row_index, merged_cell_index))\n",
    "\n",
    "                # Append '**' to the text of the cell if not already processed\n",
    "                if cell.text.strip():  # Check if the cell is not empty\n",
    "#                    cell.text += '*T*B*L'\n",
    "                    for para in cell.paragraphs:\n",
    "                        # Add an asterisk (*) to the end of each cell paragraph\n",
    "                        print(para.text)\n",
    "                        # Just a small check to see that we processed this\n",
    "                        #para.add_run('*T')\n",
    "\n",
    "                        # Store the original formatting\n",
    "                        original_runs = para.runs.copy()                        \n",
    "                        # let us call the llm \n",
    "                        llm_output_text = fetch_llm_response(para.text)\n",
    "\n",
    "                        # edits, corrections = document_postprocessing(llm_output_text)\n",
    "                        edits, corrections = process_llm_output(llm_output_text) \n",
    "                        \n",
    "                        #let us reapply formatting\n",
    "                        \n",
    "                        # Clear the paragraph and add the edited text\n",
    "                        para.clear()\n",
    "                        #para.add_run(edited_text['choices'][0]['text'].strip())\n",
    "                        para.add_run(edits)\n",
    "                        # Attempt to reapply formatting\n",
    "                        new_runs = para.runs\n",
    "                        for i, run in enumerate(new_runs):\n",
    "                            if i < len(original_runs):\n",
    "                                run.font.name = original_runs[i].font.name\n",
    "                                run.font.size = original_runs[i].font.size\n",
    "                                run.font.bold = original_runs[i].font.bold\n",
    "                                run.font.italic = original_runs[i].font.italic\n",
    "                                # Add more formatting attributes as needed                                               \n",
    "\n",
    "                        # Let us log the corrections made\n",
    "                        corrections_doc.add_paragraph()\n",
    "                        corrections_doc.add_paragraph(f\"Original Text : \\n {para.text}\")\n",
    "                        corrections_doc.add_paragraph(f\"Corrections: \\n{llm_output_text}\")\n",
    "                        \n",
    "            print()  # Newline after each row\n",
    "    \n",
    "    \n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "def proofread_document(input_file, output_file, correction_file):\n",
    "    # Load the document\n",
    "    input_doc = Document(input_file)\n",
    "    corrections_doc = Document()\n",
    "    corrections_doc.add_heading(\"Corrections Made\", 0)\n",
    "    modified_doc = deepcopy(input_doc)\n",
    "    modified_doc, corrections_doc = process_document_paragraphs(modified_doc, corrections_doc)\n",
    "    modified_doc, corrections_doc = process_document_tables(modified_doc, corrections_doc)\n",
    "    # Save the edited document\n",
    "    modified_doc.save(output_file)\n",
    "    corrections_doc.save(correction_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99e35ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 126 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      13.61 ms /    20 runs   (    0.68 ms per token,  1469.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    4504.72 ms /    20 runs   (  225.24 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:       total time =    4535.29 ms /    20 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'HEADING'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: HEADING\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "HEADING\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " HEADING\n",
      "CORRECTIONS: \n",
      " None needed.\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      15.85 ms /    23 runs   (    0.69 ms per token,  1450.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2169.47 ms /    72 tokens (   30.13 ms per token,    33.19 tokens per second)\n",
      "llama_print_timings:        eval time =    5003.02 ms /    22 runs   (  227.41 ms per token,     4.40 tokens per second)\n",
      "llama_print_timings:       total time =    7208.47 ms /    94 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 75 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'SECTION 1 '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'SECTION 1'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "'SECTION 1'\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " 'SECTION 1'\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      16.02 ms /    24 runs   (    0.67 ms per token,  1497.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2265.24 ms /    75 tokens (   30.20 ms per token,    33.11 tokens per second)\n",
      "llama_print_timings:        eval time =    5174.54 ms /    23 runs   (  224.98 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:       total time =    7476.74 ms /    98 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'I am trying something bold here. '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: I am trying something bold here.\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "I am trying something bold here.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " I am trying something bold here.\n",
      "CORRECTIONS: \n",
      " None needed.\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      15.59 ms /    24 runs   (    0.65 ms per token,  1539.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2160.33 ms /    72 tokens (   30.00 ms per token,    33.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5155.44 ms /    23 runs   (  224.15 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    7351.78 ms /    95 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'SECTION 2 '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: SECTION 2\n",
      "2. Corrections: No formatting errors found.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "SECTION 2\n",
      "\n",
      "Corrections:\n",
      "No formatting errors found.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " SECTION 2\n",
      "CORRECTIONS: \n",
      " No formatting errors found.\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      12.67 ms /    19 runs   (    0.67 ms per token,  1499.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2007.74 ms /    70 tokens (   28.68 ms per token,    34.87 tokens per second)\n",
      "llama_print_timings:        eval time =    4037.45 ms /    18 runs   (  224.30 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    6073.88 ms /    88 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 127 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Product Details'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Product Details\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Product Details\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Product Details\n",
      "CORRECTIONS: \n",
      " None needed.\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      72.05 ms /   106 runs   (    0.68 ms per token,  1471.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3696.96 ms /   127 tokens (   29.11 ms per token,    34.35 tokens per second)\n",
      "llama_print_timings:        eval time =   23606.03 ms /   105 runs   (  224.82 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:       total time =   27473.94 ms /   232 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 126 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'I saw a smaple product. These products is below expected quality. \\n I start to understand what he said is quite     right.  The correct value is 12.5  % and 3   r d position. The time is 10:30 am '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'I saw a sample product. These products are below expected quality. I start to understand what he said is quite right. The correct value is 12.5% and the 3rd position. The time is 10:30 am.'\n",
      "2. Corrections: [Your corrected text here]\n",
      "- Replace 'smaple' with 'sample'.\n",
      "- Remove extra space after '%'.\n",
      "- Add a period at the end of the sentence.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "'I saw a sample product. These products are below expected quality. I start to understand what he said is quite right. The correct value is 12.5% and the 3rd position. The time is 10:30 am.'\n",
      "\n",
      "Corrections:\n",
      "[Your corrected text here]\n",
      "- Replace 'smaple' with 'sample'.\n",
      "- Remove extra space after '%'.\n",
      "- Add a period at the end of the sentence.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " 'I saw a sample product. These products are below expected quality. I start to understand what he said is quite right. The correct value is 12.5% and the 3rd position. The time is 10:30 am.'\n",
      "CORRECTIONS: \n",
      " [Your corrected text here]\n",
      "- Replace 'smaple' with 'sample'.\n",
      "- Remove extra space after '%'.\n",
      "- Add a period at the end of the sentence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      49.48 ms /    73 runs   (    0.68 ms per token,  1475.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3612.94 ms /   126 tokens (   28.67 ms per token,    34.87 tokens per second)\n",
      "llama_print_timings:        eval time =   16247.22 ms /    72 runs   (  225.66 ms per token,     4.43 tokens per second)\n",
      "llama_print_timings:       total time =   19975.02 ms /   198 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '                   The length of the object is 845mm and cost $1m. The sunrise happens at 0750 EST and visibily is 35 km.      The right way to summarize and favor some advisors is still to be found'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibility is 35 km. The right way to summarize and favor some advisors is still to be found.'\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibility is 35 km. The right way to summarize and favor some advisors is still to be found.'\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibility is 35 km. The right way to summarize and favor some advisors is still to be found.'\n",
      "CORRECTIONS: \n",
      " [No corrections needed]\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      12.94 ms /    19 runs   (    0.68 ms per token,  1467.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2113.51 ms /    70 tokens (   30.19 ms per token,    33.12 tokens per second)\n",
      "llama_print_timings:        eval time =    4070.32 ms /    18 runs   (  226.13 ms per token,     4.42 tokens per second)\n",
      "llama_print_timings:       total time =    6212.60 ms /    88 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 128 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Table Section'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Table Section\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Table Section\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Table Section\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      84.64 ms /   121 runs   (    0.70 ms per token,  1429.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3599.23 ms /   128 tokens (   28.12 ms per token,    35.56 tokens per second)\n",
      "llama_print_timings:        eval time =   27011.23 ms /   120 runs   (  225.09 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:       total time =   30808.45 ms /   248 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Some table to be addd latr here by I. I am trying t  o  make a long  setence to see if the LLM is able to capture some of these                       errors such as 2    nd  or 4    th or 6.5   % '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Some table to be added later here by I. I am trying t o make a long setence to see if the LLM is able to capture some of these errors such as 2 nd or 4 th or 6.5 %.'\n",
      "2. Corrections: [1] Remove extra space after \"Some table\" and before \"to be added later here by I.\" [2] Change \"t o make a long setence\" to \"to create a long sentence.\" [3] Add missing period at the end of the sentence.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "'Some table to be added later here by I. I am trying t o make a long setence to see if the LLM is able to capture some of these errors such as 2 nd or 4 th or 6.5 %.'\n",
      "\n",
      "Corrections:\n",
      "[1] Remove extra space after \"Some table\" and before \"to be added later here by I.\" [2] Change \"t o make a long setence\" to \"to create a long sentence.\" [3] Add missing period at the end of the sentence.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " 'Some table to be added later here by I. I am trying t o make a long setence to see if the LLM is able to capture some of these errors such as 2 nd or 4 th or 6.5 %.'\n",
      "CORRECTIONS: \n",
      " [1] Remove extra space after \"Some table\" and before \"to be added later here by I.\" [2] Change \"t o make a long setence\" to \"to create a long sentence.\" [3] Add missing period at the end of the sentence.\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n",
      "IN Table\n",
      "Spelling Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      24.46 ms /    37 runs   (    0.66 ms per token,  1512.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2098.54 ms /    72 tokens (   29.15 ms per token,    34.31 tokens per second)\n",
      "llama_print_timings:        eval time =    8060.79 ms /    36 runs   (  223.91 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   10214.24 ms /   108 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Spelling Errors'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Spelling Errors'\n",
      "2. Corrections: \n",
      "   - Remove extra space after the colon in \"Spelling Errors\"\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "'Spelling Errors'\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after the colon in \"Spelling Errors\"\n",
      "DONE process_llm_output\n",
      "Grammar Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      26.12 ms /    39 runs   (    0.67 ms per token,  1493.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2373.06 ms /    73 tokens (   32.51 ms per token,    30.76 tokens per second)\n",
      "llama_print_timings:        eval time =    8553.36 ms /    38 runs   (  225.09 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:       total time =   10985.28 ms /   111 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 76 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Grammar Errors'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Grammar Errors'\n",
      "2. Corrections: \n",
      "   - Remove extra space after the colon in \"Grammar Errors\".\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "'Grammar Errors'\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after the colon in \"Grammar Errors\".\n",
      "DONE process_llm_output\n",
      "\n",
      "This is a mistke in spelling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      24.22 ms /    36 runs   (    0.67 ms per token,  1486.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2218.40 ms /    76 tokens (   29.19 ms per token,    34.26 tokens per second)\n",
      "llama_print_timings:        eval time =    7823.37 ms /    35 runs   (  223.52 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   10097.87 ms /   111 tokens\n",
      "Llama.generate: 59 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'This is a mistke in spelling'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'This is a mistake in spelling'\n",
      "2. Corrections: \n",
      "   - Remove extra space after \"mistake\"\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "'This is a mistake in spelling'\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after \"mistake\"\n",
      "DONE process_llm_output\n",
      "This is examples of grammar mistakes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      30.96 ms /    47 runs   (    0.66 ms per token,  1518.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2413.91 ms /    73 tokens (   33.07 ms per token,    30.24 tokens per second)\n",
      "llama_print_timings:        eval time =   10352.74 ms /    46 runs   (  225.06 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:       total time =   12837.63 ms /   119 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 78 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'This is examples of grammar mistakes'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: This is an example of grammar mistakes.\n",
      "2. Corrections: \n",
      "   - Remove \"of\" after \"examples\".\n",
      "   - Change \"is\" to \"are\".\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "This is an example of grammar mistakes.\n",
      "\n",
      "Corrections:\n",
      "- Remove \"of\" after \"examples\".\n",
      "   - Change \"is\" to \"are\".\n",
      "DONE process_llm_output\n",
      "\n",
      "The wether was beautifull on our vacation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      35.25 ms /    53 runs   (    0.67 ms per token,  1503.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2248.12 ms /    78 tokens (   28.82 ms per token,    34.70 tokens per second)\n",
      "llama_print_timings:        eval time =   11670.17 ms /    52 runs   (  224.43 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   13999.66 ms /   130 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 76 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'The wether was beautifull on our vacation.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: The weather was beautiful on our vacation.\n",
      "2. Corrections: \n",
      "   - Replace \"wether\" with \"weather.\"\n",
      "   - Remove extra space after the period at the end of the sentence.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "The weather was beautiful on our vacation.\n",
      "\n",
      "Corrections:\n",
      "- Replace \"wether\" with \"weather.\"\n",
      "   - Remove extra space after the period at the end of the sentence.\n",
      "DONE process_llm_output\n",
      "Me and her went to the store yesterday.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      29.87 ms /    45 runs   (    0.66 ms per token,  1506.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2303.66 ms /    76 tokens (   30.31 ms per token,    32.99 tokens per second)\n",
      "llama_print_timings:        eval time =    9860.12 ms /    44 runs   (  224.09 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   12231.37 ms /   120 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 82 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Me and her went to the store yesterday.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Me and her went to the store yesterday.\n",
      "2. Corrections: \n",
      "   - Remove extra space after \"Me\"\n",
      "   - Change \"her\" to \"her.\"\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Me and her went to the store yesterday.\n",
      "\n",
      "Corrections:\n",
      "- Remove extra space after \"Me\"\n",
      "   - Change \"her\" to \"her.\"\n",
      "DONE process_llm_output\n",
      "\n",
      "She recieved an accolade for her excelent work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      38.16 ms /    57 runs   (    0.67 ms per token,  1493.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2428.45 ms /    82 tokens (   29.62 ms per token,    33.77 tokens per second)\n",
      "llama_print_timings:        eval time =   12577.08 ms /    56 runs   (  224.59 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:       total time =   15091.84 ms /   138 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 79 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'She recieved an accolade for her excelent work.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'She received an accolade for her excellent work.'\n",
      "2. Corrections: \n",
      "   - Replace \"recieved\" with \"received\" (spelling error)\n",
      "   - Remove extra space after \"work.\"\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "'She received an accolade for her excellent work.'\n",
      "\n",
      "Corrections:\n",
      "- Replace \"recieved\" with \"received\" (spelling error)\n",
      "   - Remove extra space after \"work.\"\n",
      "DONE process_llm_output\n",
      " If I would have known, I would have came earlier.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3776.62 ms\n",
      "llama_print_timings:      sample time =      41.26 ms /    61 runs   (    0.68 ms per token,  1478.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2334.78 ms /    79 tokens (   29.55 ms per token,    33.84 tokens per second)\n",
      "llama_print_timings:        eval time =   13498.09 ms /    60 runs   (  224.97 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:       total time =   15927.16 ms /   139 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  ' If I would have known, I would have came earlier.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'If I would have known, I would have come earlier.'\n",
      "2. Corrections: \n",
      "   - Replace \"would\" with \"could\" (correct verb tense)\n",
      "   - Remove extra space after comma (proper punctuation)\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "'If I would have known, I would have come earlier.'\n",
      "\n",
      "Corrections:\n",
      "- Replace \"would\" with \"could\" (correct verb tense)\n",
      "   - Remove extra space after comma (proper punctuation)\n",
      "DONE process_llm_output\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#input_file = \"/home/cdsw/data/simple-word-file-2.docx\"\n",
    "input_file = \"/home/cdsw/data/simple-word-file-with-table.docx\"\n",
    "output_file = \"/home/cdsw/data/output_doc.docx\"\n",
    "correction_file = \"/home/cdsw/data/correction_file.docx\"\n",
    "proofread_document(input_file, output_file, correction_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da729b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(input_file, output_file):\n",
    "      # Open the input Word document\n",
    "    doc = Document(input_file)\n",
    "    \n",
    "    for element in doc.paragraphs:\n",
    "        print(\"IN PURE PARAGRAPH\")\n",
    "        print(element.text)\n",
    "        element.add_run('*P')\n",
    "    \n",
    "    # Iterate through all tables in the document\n",
    "    for table in doc.tables:\n",
    "        print(\"IN Table\")        \n",
    "        printed_cells = set()  # To keep track of cells that have been processed\n",
    "        for r_index, row in enumerate(table.rows):\n",
    "            for c_index, cell in enumerate(row.cells):\n",
    "                cell_id = (r_index, c_index)  # Unique identifier for the cell\n",
    "                \n",
    "                # Skip this cell if it is already processed as part of a merged cell\n",
    "                if cell_id in printed_cells:\n",
    "                    continue\n",
    "\n",
    "                # Detect merged cells\n",
    "                is_merged = False\n",
    "                for other_cell in row.cells:\n",
    "                    if other_cell is not cell and other_cell._element is cell._element:\n",
    "                        is_merged = True\n",
    "                        break\n",
    "\n",
    "                # If it's a merged cell, avoid processing duplicates\n",
    "                if is_merged:\n",
    "                    # Register this cell's element to skip duplicates\n",
    "                    for merged_row_index, merged_row in enumerate(table.rows):\n",
    "                        for merged_cell_index, merged_cell in enumerate(merged_row.cells):\n",
    "                            if merged_cell._element is cell._element:\n",
    "                                printed_cells.add((merged_row_index, merged_cell_index))\n",
    "\n",
    "                # Append '**' to the text of the cell if not already processed\n",
    "                if cell.text.strip():  # Check if the cell is not empty\n",
    "#                    cell.text += '*T*B*L'\n",
    "                    for paragraph in cell.paragraphs:\n",
    "                        # Add an asterisk (*)   to the end of each cell paragraph\n",
    "                        print(paragraph.text)\n",
    "                        paragraph.add_run('*T')\n",
    "\n",
    "            print()  # Newline after each row\n",
    "\n",
    "    # Save the modified document to the output file\n",
    "    doc.save(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5bc3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      13.54 ms /    20 runs   (    0.68 ms per token,  1477.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3652.78 ms /   125 tokens (   29.22 ms per token,    34.22 tokens per second)\n",
      "llama_print_timings:        eval time =    4100.43 ms /    19 runs   (  215.81 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:       total time =    7784.64 ms /   144 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'HEADING'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: HEADING\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      15.20 ms /    23 runs   (    0.66 ms per token,  1512.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2041.48 ms /    70 tokens (   29.16 ms per token,    34.29 tokens per second)\n",
      "llama_print_timings:        eval time =    4749.94 ms /    22 runs   (  215.91 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:       total time =    6826.65 ms /    92 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'SECTION 1 '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: 'SECTION 1'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      16.55 ms /    25 runs   (    0.66 ms per token,  1510.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2205.26 ms /    73 tokens (   30.21 ms per token,    33.10 tokens per second)\n",
      "llama_print_timings:        eval time =    5223.78 ms /    24 runs   (  217.66 ms per token,     4.59 tokens per second)\n",
      "llama_print_timings:       total time =    7466.42 ms /    97 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'I am trying something bold here. '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: 'I am trying something bold here.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      14.44 ms /    22 runs   (    0.66 ms per token,  1523.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2520.20 ms /    70 tokens (   36.00 ms per token,    27.78 tokens per second)\n",
      "llama_print_timings:        eval time =    4552.72 ms /    21 runs   (  216.80 ms per token,     4.61 tokens per second)\n",
      "llama_print_timings:       total time =    7105.25 ms /    91 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'SECTION 2 '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: SECTION 2\n",
      "2. Corrections: No corrections needed.\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      14.43 ms /    22 runs   (    0.66 ms per token,  1524.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1988.19 ms /    68 tokens (   29.24 ms per token,    34.20 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.71 ms /    21 runs   (  215.99 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:       total time =    6556.68 ms /    89 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 125 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Product Details'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " **.1. Edited Text:** Product Details\n",
      "**.2. Corrections:** No corrections needed.\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      98.38 ms /   139 runs   (    0.71 ms per token,  1412.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3658.11 ms /   125 tokens (   29.26 ms per token,    34.17 tokens per second)\n",
      "llama_print_timings:        eval time =   30722.95 ms /   138 runs   (  222.63 ms per token,     4.49 tokens per second)\n",
      "llama_print_timings:       total time =   34625.03 ms /   263 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 124 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'I saw a smaple product. These products is below expected quality. \\n I start to understand what he said is quite     right.  The correct value is 12.5  % and 3   r d position. The time is 10:30 am '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: 'I saw a sample product. These products are below expected quality. I start to understand what he said is quite right. The correct value is 12.5% and the third position. The time is 10:30 am.'\n",
      "2. Corrections: [list each correction made]\n",
      "   - Replaced \"smaple\" with \"sample\"\n",
      "   - Changed \"These products is below expected quality.\" to \"These products are below expected quality.\"\n",
      "   - Removed extra space after \"%\" and \"3 r d position.\"\n",
      "   - Added a period at the end of the sentence.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      78.28 ms /   113 runs   (    0.69 ms per token,  1443.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3789.71 ms /   124 tokens (   30.56 ms per token,    32.72 tokens per second)\n",
      "llama_print_timings:        eval time =   24296.99 ms /   112 runs   (  216.94 ms per token,     4.61 tokens per second)\n",
      "llama_print_timings:       total time =   28271.72 ms /   236 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '                   The length of the object is 845mm and cost $1m. The sunrise happens at 0750 EST and visibily is 35 km.      The right way to summarize and favor some advisors is still to be found'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " **.1. Edited Text:** The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibility is 35 km. The right way to summarize and favor some advisors is still to be found.'\n",
      "\n",
      "**2. Corrections:**\n",
      "- Removed extra space after 'The length of the object'\n",
      "- Changed 'cost $1m' to '$1m cost'\n",
      "- Added a period at the end of the sentence\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      12.77 ms /    19 runs   (    0.67 ms per token,  1487.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1959.95 ms /    68 tokens (   28.82 ms per token,    34.69 tokens per second)\n",
      "llama_print_timings:        eval time =    3942.65 ms /    18 runs   (  219.04 ms per token,     4.57 tokens per second)\n",
      "llama_print_timings:       total time =    5930.68 ms /    86 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 129 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Table Section'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: Table Section\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      98.86 ms /   140 runs   (    0.71 ms per token,  1416.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3729.61 ms /   129 tokens (   28.91 ms per token,    34.59 tokens per second)\n",
      "llama_print_timings:        eval time =   30118.07 ms /   139 runs   (  216.68 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:       total time =   34083.63 ms /   268 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Some table to be addd latr here by I. I am trying t  o  make a looooong  setence to see if the LLM is able to capture some of these                       errors such as 2 n    d  or 4 t  h or 6 %s '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: 'Some table to be added later here by I. I am trying t o make a looooong setence to see if the LLM is able to capture some of these errors such as 2 n, d or 4 t h or 6 %s'\n",
      "2. Corrections: [list each correction made]\n",
      "- Replaced 't' with 'to' in \"I am trying t o make a looooong setence\"\n",
      "- Removed extra space after 'by I.'\n",
      "- Changed 'd' to 'and' in \"Some table to be added later here by I. and...\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/home/cdsw/data/simple-word-file-2.docx\"\n",
    "output_file = \"/home/cdsw/data/output_doc.docx\"\n",
    "correction_file = \"/home/cdsw/data/correction_file.docx\"\n",
    "proofread_and_edit_paragraphs(input_file, output_file, correction_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
