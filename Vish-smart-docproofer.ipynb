{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361aa329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -q open-webui python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab8a9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/cdsw/.cache/huggingface/hub/models--TheBloke--Karen_TheEditor_V2_STRICT_Mistral_7B-GGUF/snapshots/6c654aa207a4b673379db7a928b87a01d644676c/karen_theeditor_v2_strict_mistral_7b.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = fpham_karen_theeditor_v2_strict_mistr...\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = fpham_karen_theeditor_v2_strict_mistral_7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7338.66 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 16384\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  1088.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'fpham_karen_theeditor_v2_strict_mistral_7b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "#imports and set up the local model\n",
    "from docx import Document\n",
    "from transformers import pipeline\n",
    "from llama_cpp import Llama\n",
    "from copy import deepcopy\n",
    "from smartdoc_utils import document_postprocessing, process_llm_output\n",
    "model_path = \"/home/cdsw/.cache/huggingface/hub/models--TheBloke--Karen_TheEditor_V2_STRICT_Mistral_7B-GGUF/snapshots/6c654aa207a4b673379db7a928b87a01d644676c/karen_theeditor_v2_strict_mistral_7b.Q8_0.gguf\"\n",
    "# Initialize the model with GPU support\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,  # -1 means use all available GPU layers\n",
    "    n_ctx=16384,  # adjust based on your GPU memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f0b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_llm_response(text):\n",
    "    #Use the LLM to proofread and edit\n",
    "    # prompt = (\n",
    "    #     \"<|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\\n\"\n",
    "    #     \"<|user|> ** TASK**  \\n\" \n",
    "    #     f\"1. Edit the following text for spelling and grammar mistakes:  '{text}'\\n\"\n",
    "    #     f\"2. Remove any formatting errors such as extra spaces \\n\"\n",
    "    #     f\"**IMPORTANT** use the following template to format your response **.\\n\"\n",
    "    #     f\"1. Edited Text: \\n\"\n",
    "    #     f\"[ Your corrected text here ]\\n\"\n",
    "    #     f\"2. Corrections: \\n\"\n",
    "    #     f\"[ list **each correction made here** ]\\n\"\n",
    "    #     \"<|end|>\\n\"\n",
    "    #     \"<|assistant|>\\n\"\n",
    "    # )\n",
    "    prompt = (\n",
    "        \"<|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\\n\"\n",
    "        \"<|user|> ** TASK**  \\n\" \n",
    "        f\"1. Edit the following text for spelling and grammar mistakes:  '{text}'\\n\"\n",
    "        f\"2. Remove any formatting errors such as extra spaces\"\n",
    "        f\"**IMPORTANT** use the following template to format your response **.\"\n",
    "        f\"1. Edited Text: \"\n",
    "        f\"[ Your corrected text here ]\"\n",
    "        f\"2. Corrections: \"\n",
    "        f\"[ Make a numbered list of your corrections ]\"\n",
    "        \"<|end|>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "    )    \n",
    "#        edited_text = llm(prompt, max_length=len(text) + 50)[0]['generated_text']\n",
    "    edited_text=llm(prompt, max_tokens=400, temperature=0.7, top_p=0.1, top_k=40, repeat_penalty=1.18)\n",
    "    response_text = edited_text['choices'][0]['text'].strip()\n",
    "    print(f\"PROMPT: \\n {prompt} \\n\\n\")\n",
    "    print(f\"LLM RESPONSE: \\n{response_text}\\n\\n\")      \n",
    "    \n",
    "    return response_text\n",
    "    \n",
    "# code that Aamir showed with language_tool checks comes here\n",
    "# Pre-process an input document to handle formatting, \n",
    "# language checks, etc. before further processing\n",
    "#\n",
    "# Args:\n",
    "#   input_doc: The original input document \n",
    "#   corrections_doc: An empty document to populate with corrections \n",
    "#\n",
    "# Returns: \n",
    "#   modified_doc: A modified version of the input document \n",
    "#   corrections_doc: Populated with any corrections made during p\n",
    "def pre_process_document(input_doc, corrections_doc): \n",
    "    \n",
    "    ## REPLACE this with logic of the modified doc\n",
    "    modified_doc = deepcopy(input_doc)\n",
    "    \n",
    "    # logic for pre-processing Fonts, Australian Language Checks, etc. \n",
    "    \n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "def process_document_paragraphs(modified_doc, corrections_doc) : \n",
    "\n",
    "    #modified_doc = deepcopy(input_doc)\n",
    "    # Process each paragraph\n",
    "    for para in modified_doc.paragraphs:\n",
    "        # Store the original formatting\n",
    "        original_runs = para.runs.copy()\n",
    "        \n",
    "        if not para.text.strip():\n",
    "            print(\"Skipping LLM CALL:\")\n",
    "            continue\n",
    "        else:\n",
    "            # Get the text content\n",
    "            text = para.text\n",
    "\n",
    "        llm_output_text = fetch_llm_response(text)\n",
    "\n",
    "        # edits, corrections = document_postprocessing(llm_output_text)\n",
    "        edits, corrections = process_llm_output(llm_output_text)\n",
    "        print(\"EDITS : \\n\", edits)\n",
    "        print(\"CORRECTIONS: \\n\", corrections)\n",
    "        # Clear the paragraph and add the edited text\n",
    "        para.clear()\n",
    "        #para.add_run(edited_text['choices'][0]['text'].strip())\n",
    "        para.add_run(edits)\n",
    "        # Attempt to reapply formatting\n",
    "        new_runs = para.runs\n",
    "        for i, run in enumerate(new_runs):\n",
    "            if i < len(original_runs):\n",
    "                run.font.name = original_runs[i].font.name\n",
    "                run.font.size = original_runs[i].font.size\n",
    "                run.font.bold = original_runs[i].font.bold\n",
    "                run.font.italic = original_runs[i].font.italic\n",
    "                run.font.color.rgb = original_runs[i].font.color.rgb\n",
    "                run.font.underline = original_runs[i].font.underline    \n",
    "                # Add more formatting attributes as needed\n",
    "        \n",
    "\n",
    "        # Let us log the corrections made\n",
    "        corrections_doc.add_paragraph()\n",
    "        corrections_doc.add_paragraph(f\"Original Text : \\n {text}\")\n",
    "        corrections_doc.add_paragraph(f\"Edits : \\n {edits}\")\n",
    "        corrections_doc.add_paragraph(f\"Corrections: \\n{llm_output_text}\")\n",
    "                   \n",
    "    \n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "    \n",
    "def process_document_tables(modified_doc, corrections_doc) : \n",
    "    \n",
    "    ## REPLACE this with logic of the modified doc\n",
    "    #modified_doc = deepcopy(input_doc)\n",
    "    # Iterate through all tables in the document\n",
    "    for table in modified_doc.tables:\n",
    "        print(\"IN Table\")        \n",
    "        printed_cells = set()  # To keep track of cells that have been processed\n",
    "        for r_index, row in enumerate(table.rows):\n",
    "            for c_index, cell in enumerate(row.cells):\n",
    "                cell_id = (r_index, c_index)  # Unique identifier for the cell\n",
    "                \n",
    "                # Skip this cell if it is already processed as part of a merged cell\n",
    "                if cell_id in printed_cells:\n",
    "                    continue\n",
    "\n",
    "                # Detect merged cells\n",
    "                is_merged = False\n",
    "                for other_cell in row.cells:\n",
    "                    if other_cell is not cell and other_cell._element is cell._element:\n",
    "                        is_merged = True\n",
    "                        break\n",
    "\n",
    "                # If it's a merged cell, avoid processing duplicates\n",
    "                if is_merged:\n",
    "                    # Register this cell's element to skip duplicates\n",
    "                    for merged_row_index, merged_row in enumerate(table.rows):\n",
    "                        for merged_cell_index, merged_cell in enumerate(merged_row.cells):\n",
    "                            if merged_cell._element is cell._element:\n",
    "                                printed_cells.add((merged_row_index, merged_cell_index))\n",
    "\n",
    "                # Append '**' to the text of the cell if not already processed\n",
    "                if cell.text.strip():  # Check if the cell is not empty\n",
    "#                    cell.text += '*T*B*L'\n",
    "                    for para in cell.paragraphs:\n",
    "                        # Add an asterisk (*) to the end of each cell paragraph\n",
    "                        print(para.text)\n",
    "                        # Just a small check to see that we processed this\n",
    "                        #para.add_run('*T')\n",
    "\n",
    "                        # Store the original formatting\n",
    "                        original_runs = para.runs.copy()                        \n",
    "                        # let us call the llm \n",
    "                        llm_output_text = fetch_llm_response(para.text)\n",
    "\n",
    "                        # edits, corrections = document_postprocessing(llm_output_text)\n",
    "                        edits, corrections = process_llm_output(llm_output_text) \n",
    "                        \n",
    "                        #let us reapply formatting\n",
    "                        \n",
    "                        # Clear the paragraph and add the edited text\n",
    "                        para.clear()\n",
    "                        #para.add_run(edited_text['choices'][0]['text'].strip())\n",
    "                        para.add_run(edits)\n",
    "                        # Attempt to reapply formatting\n",
    "                        new_runs = para.runs\n",
    "                        for i, run in enumerate(new_runs):\n",
    "                            if i < len(original_runs):\n",
    "                                run.font.name = original_runs[i].font.name\n",
    "                                run.font.size = original_runs[i].font.size\n",
    "                                run.font.bold = original_runs[i].font.bold\n",
    "                                run.font.italic = original_runs[i].font.italic\n",
    "                                run.font.color.rgb = original_runs[i].font.color.rgb \n",
    "                                run.font.underline = original_runs[i].font.underline                                                                   \n",
    "                                # Add more formatting attributes as needed                                               \n",
    "\n",
    "                        # Let us log the corrections made\n",
    "                        corrections_doc.add_paragraph()\n",
    "                        corrections_doc.add_paragraph(f\"Original Text : \\n {para.text}\")\n",
    "                        corrections_doc.add_paragraph(f\"Corrections: \\n{llm_output_text}\")\n",
    "                        \n",
    "            print()  # Newline after each row\n",
    "    \n",
    "    \n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "def proofread_document(input_file, output_file, correction_file):\n",
    "    # Load the document\n",
    "    input_doc = Document(input_file)\n",
    "    corrections_doc = Document()\n",
    "    corrections_doc.add_heading(\"Corrections Made\", 0)\n",
    "    modified_doc = deepcopy(input_doc)\n",
    "    modified_doc, corrections_doc = process_document_paragraphs(modified_doc, corrections_doc)\n",
    "    modified_doc, corrections_doc = process_document_tables(modified_doc, corrections_doc)\n",
    "    # Save the edited document\n",
    "    modified_doc.save(output_file)\n",
    "    corrections_doc.save(correction_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99e35ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3614.48 ms\n",
      "llama_print_timings:      sample time =      13.13 ms /    20 runs   (    0.66 ms per token,  1522.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3614.08 ms /   127 tokens (   28.46 ms per token,    35.14 tokens per second)\n",
      "llama_print_timings:        eval time =    4260.09 ms /    19 runs   (  224.22 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =    7907.18 ms /   146 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'HEADING'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: HEADING\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "No edited text found.\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " No edited text found.\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "property 'color' of 'Font' object has no setter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/cdsw/data/output_doc.docx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m correction_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/cdsw/data/correction_file.docx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mproofread_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrection_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 185\u001b[0m, in \u001b[0;36mproofread_document\u001b[0;34m(input_file, output_file, correction_file)\u001b[0m\n\u001b[1;32m    183\u001b[0m corrections_doc\u001b[38;5;241m.\u001b[39madd_heading(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrections Made\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    184\u001b[0m modified_doc \u001b[38;5;241m=\u001b[39m deepcopy(input_doc)\n\u001b[0;32m--> 185\u001b[0m modified_doc, corrections_doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_document_paragraphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodified_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrections_doc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m modified_doc, corrections_doc \u001b[38;5;241m=\u001b[39m process_document_tables(modified_doc, corrections_doc)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Save the edited document\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 90\u001b[0m, in \u001b[0;36mprocess_document_paragraphs\u001b[0;34m(modified_doc, corrections_doc)\u001b[0m\n\u001b[1;32m     88\u001b[0m         run\u001b[38;5;241m.\u001b[39mfont\u001b[38;5;241m.\u001b[39mbold \u001b[38;5;241m=\u001b[39m original_runs[i]\u001b[38;5;241m.\u001b[39mfont\u001b[38;5;241m.\u001b[39mbold\n\u001b[1;32m     89\u001b[0m         run\u001b[38;5;241m.\u001b[39mfont\u001b[38;5;241m.\u001b[39mitalic \u001b[38;5;241m=\u001b[39m original_runs[i]\u001b[38;5;241m.\u001b[39mfont\u001b[38;5;241m.\u001b[39mitalic\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfont\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor\u001b[49m \u001b[38;5;241m=\u001b[39m original_runs[i]\u001b[38;5;241m.\u001b[39mfont\u001b[38;5;241m.\u001b[39mcolor\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;66;03m# Add more formatting attributes as needed\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Let us log the corrections made\u001b[39;00m\n\u001b[1;32m     95\u001b[0m corrections_doc\u001b[38;5;241m.\u001b[39madd_paragraph()\n",
      "\u001b[0;31mAttributeError\u001b[0m: property 'color' of 'Font' object has no setter"
     ]
    }
   ],
   "source": [
    "#input_file = \"/home/cdsw/data/simple-word-file-2.docx\"\n",
    "input_file = \"/home/cdsw/data/simple-word-file-with-table.docx\"\n",
    "output_file = \"/home/cdsw/data/output_doc.docx\"\n",
    "correction_file = \"/home/cdsw/data/correction_file.docx\"\n",
    "proofread_document(input_file, output_file, correction_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da729b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(input_file, output_file):\n",
    "      # Open the input Word document\n",
    "    doc = Document(input_file)\n",
    "    \n",
    "    for element in doc.paragraphs:\n",
    "        print(\"IN PURE PARAGRAPH\")\n",
    "        print(element.text)\n",
    "        element.add_run('*P')\n",
    "    \n",
    "    # Iterate through all tables in the document\n",
    "    for table in doc.tables:\n",
    "        print(\"IN Table\")        \n",
    "        printed_cells = set()  # To keep track of cells that have been processed\n",
    "        for r_index, row in enumerate(table.rows):\n",
    "            for c_index, cell in enumerate(row.cells):\n",
    "                cell_id = (r_index, c_index)  # Unique identifier for the cell\n",
    "                \n",
    "                # Skip this cell if it is already processed as part of a merged cell\n",
    "                if cell_id in printed_cells:\n",
    "                    continue\n",
    "\n",
    "                # Detect merged cells\n",
    "                is_merged = False\n",
    "                for other_cell in row.cells:\n",
    "                    if other_cell is not cell and other_cell._element is cell._element:\n",
    "                        is_merged = True\n",
    "                        break\n",
    "\n",
    "                # If it's a merged cell, avoid processing duplicates\n",
    "                if is_merged:\n",
    "                    # Register this cell's element to skip duplicates\n",
    "                    for merged_row_index, merged_row in enumerate(table.rows):\n",
    "                        for merged_cell_index, merged_cell in enumerate(merged_row.cells):\n",
    "                            if merged_cell._element is cell._element:\n",
    "                                printed_cells.add((merged_row_index, merged_cell_index))\n",
    "\n",
    "                # Append '**' to the text of the cell if not already processed\n",
    "                if cell.text.strip():  # Check if the cell is not empty\n",
    "#                    cell.text += '*T*B*L'\n",
    "                    for paragraph in cell.paragraphs:\n",
    "                        # Add an asterisk (*)   to the end of each cell paragraph\n",
    "                        print(paragraph.text)\n",
    "                        paragraph.add_run('*T')\n",
    "\n",
    "            print()  # Newline after each row\n",
    "\n",
    "    # Save the modified document to the output file\n",
    "    doc.save(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5bc3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      13.54 ms /    20 runs   (    0.68 ms per token,  1477.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3652.78 ms /   125 tokens (   29.22 ms per token,    34.22 tokens per second)\n",
      "llama_print_timings:        eval time =    4100.43 ms /    19 runs   (  215.81 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:       total time =    7784.64 ms /   144 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'HEADING'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: HEADING\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      15.20 ms /    23 runs   (    0.66 ms per token,  1512.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2041.48 ms /    70 tokens (   29.16 ms per token,    34.29 tokens per second)\n",
      "llama_print_timings:        eval time =    4749.94 ms /    22 runs   (  215.91 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:       total time =    6826.65 ms /    92 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'SECTION 1 '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: 'SECTION 1'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      16.55 ms /    25 runs   (    0.66 ms per token,  1510.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2205.26 ms /    73 tokens (   30.21 ms per token,    33.10 tokens per second)\n",
      "llama_print_timings:        eval time =    5223.78 ms /    24 runs   (  217.66 ms per token,     4.59 tokens per second)\n",
      "llama_print_timings:       total time =    7466.42 ms /    97 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'I am trying something bold here. '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: 'I am trying something bold here.'\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      14.44 ms /    22 runs   (    0.66 ms per token,  1523.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2520.20 ms /    70 tokens (   36.00 ms per token,    27.78 tokens per second)\n",
      "llama_print_timings:        eval time =    4552.72 ms /    21 runs   (  216.80 ms per token,     4.61 tokens per second)\n",
      "llama_print_timings:       total time =    7105.25 ms /    91 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'SECTION 2 '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: SECTION 2\n",
      "2. Corrections: No corrections needed.\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      14.43 ms /    22 runs   (    0.66 ms per token,  1524.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1988.19 ms /    68 tokens (   29.24 ms per token,    34.20 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.71 ms /    21 runs   (  215.99 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:       total time =    6556.68 ms /    89 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 125 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Product Details'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " **.1. Edited Text:** Product Details\n",
      "**.2. Corrections:** No corrections needed.\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      98.38 ms /   139 runs   (    0.71 ms per token,  1412.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3658.11 ms /   125 tokens (   29.26 ms per token,    34.17 tokens per second)\n",
      "llama_print_timings:        eval time =   30722.95 ms /   138 runs   (  222.63 ms per token,     4.49 tokens per second)\n",
      "llama_print_timings:       total time =   34625.03 ms /   263 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 124 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'I saw a smaple product. These products is below expected quality. \\n I start to understand what he said is quite     right.  The correct value is 12.5  % and 3   r d position. The time is 10:30 am '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: 'I saw a sample product. These products are below expected quality. I start to understand what he said is quite right. The correct value is 12.5% and the third position. The time is 10:30 am.'\n",
      "2. Corrections: [list each correction made]\n",
      "   - Replaced \"smaple\" with \"sample\"\n",
      "   - Changed \"These products is below expected quality.\" to \"These products are below expected quality.\"\n",
      "   - Removed extra space after \"%\" and \"3 r d position.\"\n",
      "   - Added a period at the end of the sentence.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      78.28 ms /   113 runs   (    0.69 ms per token,  1443.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3789.71 ms /   124 tokens (   30.56 ms per token,    32.72 tokens per second)\n",
      "llama_print_timings:        eval time =   24296.99 ms /   112 runs   (  216.94 ms per token,     4.61 tokens per second)\n",
      "llama_print_timings:       total time =   28271.72 ms /   236 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  '                   The length of the object is 845mm and cost $1m. The sunrise happens at 0750 EST and visibily is 35 km.      The right way to summarize and favor some advisors is still to be found'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " **.1. Edited Text:** The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibility is 35 km. The right way to summarize and favor some advisors is still to be found.'\n",
      "\n",
      "**2. Corrections:**\n",
      "- Removed extra space after 'The length of the object'\n",
      "- Changed 'cost $1m' to '$1m cost'\n",
      "- Added a period at the end of the sentence\n",
      "\n",
      "\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      12.77 ms /    19 runs   (    0.67 ms per token,  1487.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1959.95 ms /    68 tokens (   28.82 ms per token,    34.69 tokens per second)\n",
      "llama_print_timings:        eval time =    3942.65 ms /    18 runs   (  219.04 ms per token,     4.57 tokens per second)\n",
      "llama_print_timings:       total time =    5930.68 ms /    86 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 129 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Table Section'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: Table Section\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3653.13 ms\n",
      "llama_print_timings:      sample time =      98.86 ms /   140 runs   (    0.71 ms per token,  1416.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3729.61 ms /   129 tokens (   28.91 ms per token,    34.59 tokens per second)\n",
      "llama_print_timings:        eval time =   30118.07 ms /   139 runs   (  216.68 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:       total time =   34083.63 ms /   268 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Some table to be addd latr here by I. I am trying t  o  make a looooong  setence to see if the LLM is able to capture some of these                       errors such as 2 n    d  or 4 t  h or 6 %s '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ list **each correction made** ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      " 1. Edited Text: 'Some table to be added later here by I. I am trying t o make a looooong setence to see if the LLM is able to capture some of these errors such as 2 n, d or 4 t h or 6 %s'\n",
      "2. Corrections: [list each correction made]\n",
      "- Replaced 't' with 'to' in \"I am trying t o make a looooong setence\"\n",
      "- Removed extra space after 'by I.'\n",
      "- Changed 'd' to 'and' in \"Some table to be added later here by I. and...\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/home/cdsw/data/simple-word-file-2.docx\"\n",
    "output_file = \"/home/cdsw/data/output_doc.docx\"\n",
    "correction_file = \"/home/cdsw/data/correction_file.docx\"\n",
    "proofread_and_edit_paragraphs(input_file, output_file, correction_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
