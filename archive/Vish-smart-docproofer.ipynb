{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361aa329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -q open-webui python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab8a9ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'smartdoc_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmartdoc_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process_llm_output\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeterministic_preprocessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeterministicPreprocessor\n\u001b[1;32m      7\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/cdsw/.cache/huggingface/hub/models--TheBloke--Karen_TheEditor_V2_STRICT_Mistral_7B-GGUF/snapshots/6c654aa207a4b673379db7a928b87a01d644676c/karen_theeditor_v2_strict_mistral_7b.Q8_0.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smartdoc_utils'"
     ]
    }
   ],
   "source": [
    "#imports and set up the local model\n",
    "from docx import Document\n",
    "from llama_cpp import Llama\n",
    "from copy import deepcopy\n",
    "from smartdoc_utils import process_llm_output\n",
    "from deterministic_preprocessor import DeterministicPreprocessor\n",
    "model_path = \"/home/cdsw/.cache/huggingface/hub/models--TheBloke--Karen_TheEditor_V2_STRICT_Mistral_7B-GGUF/snapshots/6c654aa207a4b673379db7a928b87a01d644676c/karen_theeditor_v2_strict_mistral_7b.Q8_0.gguf\"\n",
    "# Initialize the model with GPU support\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,  # -1 means use all available GPU layers\n",
    "    n_ctx=16384,  # adjust based on your GPU memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f0b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_llm_response(text):\n",
    "    #Use the LLM to proofread and edit\n",
    "    # prompt = (\n",
    "    #     \"<|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\\n\"\n",
    "    #     \"<|user|> ** TASK**  \\n\" \n",
    "    #     f\"1. Edit the following text for spelling and grammar mistakes:  '{text}'\\n\"\n",
    "    #     f\"2. Remove any formatting errors such as extra spaces \\n\"\n",
    "    #     f\"**IMPORTANT** use the following template to format your response **.\\n\"\n",
    "    #     f\"1. Edited Text: \\n\"\n",
    "    #     f\"[ Your corrected text here ]\\n\"\n",
    "    #     f\"2. Corrections: \\n\"\n",
    "    #     f\"[ list **each correction made here** ]\\n\"\n",
    "    #     \"<|end|>\\n\"\n",
    "    #     \"<|assistant|>\\n\"\n",
    "    # )\n",
    "    prompt = (\n",
    "        \"<|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\\n\"\n",
    "        \"<|user|> ** TASK**  \\n\" \n",
    "        f\"1. Edit the following text for spelling and grammar mistakes:  '{text}'\\n\"\n",
    "        f\"2. Remove any formatting errors such as extra spaces\"\n",
    "        f\"**IMPORTANT** use the following template to format your response **.\"\n",
    "        f\"1. Edited Text: \"\n",
    "        f\"[ Your corrected text here ]\"\n",
    "        f\"2. Corrections: \"\n",
    "        f\"[ Make a numbered list of your corrections ]\"\n",
    "        \"<|end|>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "    )    \n",
    "#        edited_text = llm(prompt, max_length=len(text) + 50)[0]['generated_text']\n",
    "    edited_text=llm(prompt, max_tokens=400, temperature=0.7, top_p=0.1, top_k=40, repeat_penalty=1.18)\n",
    "    response_text = edited_text['choices'][0]['text'].strip()\n",
    "    print(f\"PROMPT: \\n {prompt} \\n\\n\")\n",
    "    print(f\"LLM RESPONSE: \\n{response_text}\\n\\n\")      \n",
    "    \n",
    "    return response_text\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# code that Aamir showed with language_tool checks comes here\n",
    "# Pre-process an input document to handle formatting, \n",
    "# language checks, etc. before further processing\n",
    "#\n",
    "# Args:\n",
    "#   input_doc: The original input document \n",
    "#   corrections_doc: An empty document to populate with corrections \n",
    "#\n",
    "# Returns: \n",
    "#   modified_doc: A modified version of the input document \n",
    "#   corrections_doc: Populated with any corrections made during p\n",
    "def pre_process_document(input_doc, corrections_doc): \n",
    "    \n",
    "\n",
    "    \n",
    "    # logic for pre-processing Fonts, Australian Language Checks, etc. \n",
    "\n",
    "    # -- Need to uncomment following 3 lines - the code has not been tested here yet!\n",
    "    preprocessor = DeterministicPreprocessor()\n",
    "    modified_doc, corrections_doc = preprocessor.pre_process_document(input_doc)\n",
    "    #preprocessor.save_documents(modified_doc, corrections_doc, file_path)\n",
    "    \n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "def process_document_paragraphs(modified_doc, corrections_doc) : \n",
    "\n",
    "    #modified_doc = deepcopy(input_doc)\n",
    "    # Process each paragraph\n",
    "    for para in modified_doc.paragraphs:\n",
    "        # Store the original formatting\n",
    "        original_runs = para.runs.copy()\n",
    "        \n",
    "        if not para.text.strip():\n",
    "            print(\"Skipping LLM CALL:\")\n",
    "            continue\n",
    "        else:\n",
    "            # Get the text content\n",
    "            text = para.text\n",
    "\n",
    "        llm_output_text = fetch_llm_response(text)\n",
    "\n",
    "        # edits, corrections = document_postprocessing(llm_output_text)\n",
    "        edits, corrections = process_llm_output(llm_output_text)\n",
    "        print(\"EDITS : \\n\", edits)\n",
    "        print(\"CORRECTIONS: \\n\", corrections)\n",
    "        # Clear the paragraph and add the edited text\n",
    "        para.clear()\n",
    "        #para.add_run(edited_text['choices'][0]['text'].strip())\n",
    "        para.add_run(edits)\n",
    "        # Attempt to reapply formatting\n",
    "        new_runs = para.runs\n",
    "        for i, run in enumerate(new_runs):\n",
    "            if i < len(original_runs):\n",
    "                run.font.name = original_runs[i].font.name\n",
    "                run.font.size = original_runs[i].font.size\n",
    "                run.font.bold = original_runs[i].font.bold\n",
    "                run.font.italic = original_runs[i].font.italic\n",
    "                run.font.color.rgb = original_runs[i].font.color.rgb\n",
    "                run.font.underline = original_runs[i].font.underline    \n",
    "                # Add more formatting attributes as needed\n",
    "        \n",
    "\n",
    "        # Let us log the corrections made\n",
    "        corrections_doc.add_paragraph()\n",
    "        corrections_doc.add_paragraph(f\"Original Text : \\n {text}\")\n",
    "        corrections_doc.add_paragraph(f\"Edits : \\n {edits}\")\n",
    "        corrections_doc.add_paragraph(f\"Corrections: \\n{llm_output_text}\")\n",
    "                   \n",
    "    \n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "    \n",
    "def process_document_tables(modified_doc, corrections_doc) : \n",
    "    \n",
    "    ## REPLACE this with logic of the modified doc\n",
    "    #modified_doc = deepcopy(input_doc)\n",
    "    # Iterate through all tables in the document\n",
    "    for table in modified_doc.tables:\n",
    "        print(\"IN Table\")        \n",
    "        printed_cells = set()  # To keep track of cells that have been processed\n",
    "        for r_index, row in enumerate(table.rows):\n",
    "            for c_index, cell in enumerate(row.cells):\n",
    "                cell_id = (r_index, c_index)  # Unique identifier for the cell\n",
    "                \n",
    "                # Skip this cell if it is already processed as part of a merged cell\n",
    "                if cell_id in printed_cells:\n",
    "                    continue\n",
    "\n",
    "                # Detect merged cells\n",
    "                is_merged = False\n",
    "                for other_cell in row.cells:\n",
    "                    if other_cell is not cell and other_cell._element is cell._element:\n",
    "                        is_merged = True\n",
    "                        break\n",
    "\n",
    "                # If it's a merged cell, avoid processing duplicates\n",
    "                if is_merged:\n",
    "                    # Register this cell's element to skip duplicates\n",
    "                    for merged_row_index, merged_row in enumerate(table.rows):\n",
    "                        for merged_cell_index, merged_cell in enumerate(merged_row.cells):\n",
    "                            if merged_cell._element is cell._element:\n",
    "                                printed_cells.add((merged_row_index, merged_cell_index))\n",
    "\n",
    "                # Append '**' to the text of the cell if not already processed\n",
    "                if cell.text.strip():  # Check if the cell is not empty\n",
    "#                    cell.text += '*T*B*L'\n",
    "                    for para in cell.paragraphs:\n",
    "                        # Add an asterisk (*) to the end of each cell paragraph\n",
    "                        print(para.text)\n",
    "                        # Just a small check to see that we processed this\n",
    "                        #para.add_run('*T')\n",
    "\n",
    "                        # Store the original formatting\n",
    "                        original_runs = para.runs.copy()                        \n",
    "                        # let us call the llm \n",
    "                        llm_output_text = fetch_llm_response(para.text)\n",
    "\n",
    "                        # edits, corrections = document_postprocessing(llm_output_text)\n",
    "                        edits, corrections = process_llm_output(llm_output_text) \n",
    "                        \n",
    "                        #let us reapply formatting\n",
    "                        \n",
    "                        # Clear the paragraph and add the edited text\n",
    "                        para.clear()\n",
    "                        #para.add_run(edited_text['choices'][0]['text'].strip())\n",
    "                        para.add_run(edits)\n",
    "                        # Attempt to reapply formatting\n",
    "                        new_runs = para.runs\n",
    "                        for i, run in enumerate(new_runs):\n",
    "                            if i < len(original_runs):\n",
    "                                run.font.name = original_runs[i].font.name\n",
    "                                run.font.size = original_runs[i].font.size\n",
    "                                run.font.bold = original_runs[i].font.bold\n",
    "                                run.font.italic = original_runs[i].font.italic\n",
    "                                run.font.color.rgb = original_runs[i].font.color.rgb \n",
    "                                run.font.underline = original_runs[i].font.underline                                                                   \n",
    "                                # Add more formatting attributes as needed                                               \n",
    "\n",
    "                        # Let us log the corrections made\n",
    "                        corrections_doc.add_paragraph()\n",
    "                        corrections_doc.add_paragraph(f\"Original Text : \\n {para.text}\")\n",
    "                        corrections_doc.add_paragraph(f\"Corrections: \\n{llm_output_text}\")\n",
    "                        \n",
    "            print()  # Newline after each row\n",
    "    \n",
    "    \n",
    "    return modified_doc, corrections_doc\n",
    "\n",
    "def proofread_document(input_file, output_file, correction_file):\n",
    "    # Load the document\n",
    "    input_doc = Document(input_file)\n",
    "    corrections_doc = Document()\n",
    "    corrections_doc.add_heading(\"Corrections Made\", 0)\n",
    "    modified_doc = deepcopy(input_doc)\n",
    "    modified_doc, corrections_doc = pre_process_document(modified_doc, corrections_doc)\n",
    "    modified_doc, corrections_doc = process_document_paragraphs(modified_doc, corrections_doc)\n",
    "    modified_doc, corrections_doc = process_document_tables(modified_doc, corrections_doc)\n",
    "    # Save the edited document\n",
    "    modified_doc.save(output_file)\n",
    "    corrections_doc.save(correction_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d99e35ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   127 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5474.87 ms /   149 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'HEADING'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Heading'\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Heading\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Heading\n",
      "CORRECTIONS: \n",
      " [No corrections needed]\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3892.56 ms /    96 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 75 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'SECTION 1 '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'SECTION 1'\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "SECTION 1\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " SECTION 1\n",
      "CORRECTIONS: \n",
      " [No corrections needed]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4143.03 ms /   101 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'I am trying something bold here. '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'I am trying something bold here.'\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "I am trying something bold here.\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " I am trying something bold here.\n",
      "CORRECTIONS: \n",
      " [No corrections needed]\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    21 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3538.85 ms /    93 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'SECTION 2 '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: SECTION 2\n",
      "2. Corrections: No corrections needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "SECTION 2\n",
      "\n",
      "Corrections:\n",
      "No corrections needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " SECTION 2\n",
      "CORRECTIONS: \n",
      " No corrections needed.\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4121.69 ms /    88 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 122 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Product Details'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Product Details\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Product Details\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Product Details\n",
      "CORRECTIONS: \n",
      " None needed.\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   122 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11180.56 ms /   223 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 123 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'I saw a sample product. These products are below expected quality. \\n I start to understand what he said is quite right.  The correct value is 12.5% and 3 r d position. The time is 10:30 am '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'I saw a sample product. These products are below expected quality. I start to understand what he said is quite right. The correct value is 12.5% and the third position. The time is 10:30 am.'\n",
      "2. Corrections: [Your corrected text here]\n",
      "- Replace 'These products' with 'The product'.\n",
      "- Remove extra space after '%'.\n",
      "- Change 'rd' to 'third'.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "I saw a sample product. These products are below expected quality. I start to understand what he said is quite right. The correct value is 12.5% and the third position. The time is 10:30 am.\n",
      "\n",
      "Corrections:\n",
      "[Your corrected text here]\n",
      "- Replace 'These products' with 'The product'.\n",
      "- Remove extra space after '%'.\n",
      "- Change 'rd' to 'third'.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " I saw a sample product. These products are below expected quality. I start to understand what he said is quite right. The correct value is 12.5% and the third position. The time is 10:30 am.\n",
      "CORRECTIONS: \n",
      " [Your corrected text here]\n",
      "- Replace 'These products' with 'The product'.\n",
      "- Remove extra space after '%'.\n",
      "- Change 'rd' to 'third'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   123 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9512.03 ms /   200 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  ' The length of the object is 845mm and cost $1m. The sunrise happens at 0750 EST and visibly is 35 km.  The right way to summarise and favour some advisors is still to be found'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibly is 35 km. The right way to summarise and favour some advisors is still to be found.'\n",
      "2. Corrections: [No formatting errors were found.]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibly is 35 km. The right way to summarise and favour some advisors is still to be found.\n",
      "\n",
      "Corrections:\n",
      "[No formatting errors were found.]\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " The length of the object is 845mm and costs $1m. The sunrise happens at 0750 EST and visibly is 35 km. The right way to summarise and favour some advisors is still to be found.\n",
      "CORRECTIONS: \n",
      " [No formatting errors were found.]\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2551.86 ms /    88 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 115 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Table Section'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: Table Section\n",
      "2. Corrections: None needed.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Table Section\n",
      "\n",
      "Corrections:\n",
      "None needed.\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Table Section\n",
      "CORRECTIONS: \n",
      " None needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9813.79 ms /   182 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Some table to be added later here by I. I am trying to make a long sentence to see if the LLM is able to capture some of these errors such as 2nd or 4th or 6.5% '\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Some table to be added later here by I. I am trying to make a long sentence to see if the LLM is able to capture some of these errors such as 2nd or 4th or 6.5%.'\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Some table to be added later here by I. I am trying to make a long sentence to see if the LLM is able to capture some of these errors such as 2nd or 4th or 6.5%.\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      "EDITS : \n",
      " Some table to be added later here by I. I am trying to make a long sentence to see if the LLM is able to capture some of these errors such as 2nd or 4th or 6.5%.\n",
      "CORRECTIONS: \n",
      " [No corrections needed]\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n",
      "Skipping LLM CALL:\n",
      "IN Table\n",
      "Spelling Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4489.55 ms /    96 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Spelling Errors'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Spelling Errors'\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Spelling Errors\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      "Grammar Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    30 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6054.35 ms /   103 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 75 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'Grammar Errors'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'Grammar Errors'\n",
      "2. Corrections: [ Make a numbered list of your corrections ]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "Grammar Errors\n",
      "\n",
      "Corrections:\n",
      "[ Make a numbered list of your corrections ]\n",
      "DONE process_llm_output\n",
      "\n",
      "This is a mistake in spelling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6334.20 ms /   100 tokens\n",
      "Llama.generate: 59 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'This is a mistake in spelling'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: This is a mistake in spelling\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "This is a mistake in spelling\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      "This is examples of grammar mistakes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    40 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5631.05 ms /   113 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 75 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'This is examples of grammar mistakes'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: This is an example of grammar mistakes.\n",
      "2. Corrections: \n",
      "   a) Remove the extra space after \"This\" and before \"is\".\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "This is an example of grammar mistakes.\n",
      "\n",
      "Corrections:\n",
      "a) Remove the extra space after \"This\" and before \"is\".\n",
      "DONE process_llm_output\n",
      "\n",
      "The whether was beautiful on our vacation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7639.85 ms /   129 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 76 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'The whether was beautiful on our vacation.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: The weather was beautiful on our vacation.\n",
      "2. Corrections: \n",
      "   a) Replace \"whether\" with \"weather\".\n",
      "   b) Remove extra space after the period at the end of the sentence.\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "The weather was beautiful on our vacation.\n",
      "\n",
      "Corrections:\n",
      "a) Replace \"whether\" with \"weather\".\n",
      "   b) Remove extra space after the period at the end of the sentence.\n",
      "DONE process_llm_output\n",
      "I and her went to the store yesterday.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    33 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5682.47 ms /   109 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 78 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'I and her went to the store yesterday.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'I and her went to the store yesterday.'\n",
      "2. Corrections: [ Make a numbered list of your corrections ]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "I and her went to the store yesterday.\n",
      "\n",
      "Corrections:\n",
      "[ Make a numbered list of your corrections ]\n",
      "DONE process_llm_output\n",
      "\n",
      "She received an accolade for her excellent work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    30 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5028.68 ms /   108 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 78 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  'She received an accolade for her excellent work.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'She received an accolade for her excellent work.'\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "She received an accolade for her excellent work.\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      " If I had known, I would have come earlier.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3602.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    30 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4552.71 ms /   108 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: \n",
      " <|system|>You are an expert editor who corrects spelling, formatting and grammatical errors<|end|>\n",
      "<|user|> ** TASK**  \n",
      "1. Edit the following text for spelling and grammar mistakes:  ' If I had known, I would have come earlier.'\n",
      "2. Remove any formatting errors such as extra spaces**IMPORTANT** use the following template to format your response **.1. Edited Text: [ Your corrected text here ]2. Corrections: [ Make a numbered list of your corrections ]<|end|>\n",
      "<|assistant|>\n",
      " \n",
      "\n",
      "\n",
      "LLM RESPONSE: \n",
      "1. Edited Text: 'If I had known, I would have come earlier.'\n",
      "2. Corrections: [No corrections needed]\n",
      "\n",
      "\n",
      "INSIDE process_llm_output\n",
      "Edited Text:\n",
      "If I had known, I would have come earlier.\n",
      "\n",
      "Corrections:\n",
      "[No corrections needed]\n",
      "DONE process_llm_output\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/home/cdsw/data/simple-word-file-with-table.docx\"\n",
    "#input_file = \"/home/cdsw/data/full_client_input_doc.docx\"\n",
    "output_file = \"/home/cdsw/data/output_doc.docx\"\n",
    "correction_file = \"/home/cdsw/data/correction_file.docx\"\n",
    "proofread_document(input_file, output_file, correction_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6c0bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_redlines.engines import XmlPowerToolsEngine\n",
    "\n",
    "wrapper = XmlPowerToolsEngine()\n",
    "\n",
    "output_redlines = wrapper.run_redline('Smartdoc Processor', input_file, output_file)\n",
    "with open('/home/cdsw/data/redline_output.docx', 'wb') as f:\n",
    "    f.write(output_redlines[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
